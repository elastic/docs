<!DOCTYPE html>
<html lang="en-us">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Apache Spark support
        | Elasticsearch for Apache Hadoop [6.4]
      | Elastic
    </title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch for Apache Hadoop [6.4]" /><link rel="up" href="reference.html" title="Elasticsearch for Apache Hadoop" /><link rel="prev" href="pig.html" title="Apache Pig support" /><link rel="next" href="storm.html" title="Apache Storm support" /><meta name="description" content="Get started with the documentation for Elasticsearch, Kibana, Logstash, Beats, X-Pack, Elastic Cloud, Elasticsearch for Apache Hadoop, and our language clients." /><meta name="DC.type" content="Learn/Docs/Elasticsearch/Apache Hadoop/6.4" /><meta name="description" content="Get started with the documentation for Elasticsearch, Kibana, Logstash, Beats, X-Pack, Elastic Cloud, Elasticsearch for Apache Hadoop, and our language clients." /><meta name="DC.subject" content="Elasticsearch" /><meta name="description" content="Get started with the documentation for Elasticsearch, Kibana, Logstash, Beats, X-Pack, Elastic Cloud, Elasticsearch for Apache Hadoop, and our language clients." /><meta name="DC.identifier" content="6.4" />
    

    
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#ffffff">
<meta name="apple-mobile-web-app-title" content="Elastic">
<meta name="application-name" content="Elastic">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/mstile-144x144.png">
<meta name="theme-color" content="#ffffff">
<!-- DC tags section -->
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" >
<link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" >



	



<!-- end DC tags -->


	<meta name="description" content="" />



	<meta name="localized" content="true" />


















		
			<meta property="og:image" content="https://www.elastic.co/static/images/elastic-logo-200.png" />
		

		

    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <!-- For third-generation iPad with high-resolution Retina display: -->
    <link rel="apple-touch-icon-precomposed" sizes="64x64" href="/favicon_64x64_16bit.png">
    <!-- For iPhone with high-resolution Retina display: -->
    <link rel="apple-touch-icon-precomposed" sizes="32x32" href="/favicon_32x32.png">
    <!-- For first- and second-generation iPad: -->
    <link rel="apple-touch-icon-precomposed" sizes="16x16" href="/favicon_16x16.png">

	 

    <!-- css -->
    <link href="/static/css/main.css" rel="stylesheet">
    
    

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/jquery.cookie.js"></script>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script type="text/javascript">
  // -------------------- set language based on browser language --------------------
  var lang = '/'+(navigator.language || navigator.userLanguage).slice(0,2)+'/',
      locales = {
        '/en':'/',
        '/de/': '/de/',
        '/fr/': '/fr/',
        '/ja/':'/jp/',
        '/ko/':'/kr/',
        '/zh/':'/cn/',
        '/pt/': '/pt/',
        '/es/': '/es/'
      },
      localeUrl = '{"relative_url_prefix":"/","code":"en-us","display_code":"en-us","url":"/guide_template"}',
      currentlocale = JSON.parse(localeUrl).relative_url_prefix;

  if($.cookie("is_lang_detected") == undefined){
    if(locales[lang] != undefined && locales[lang] != currentlocale){
      locale_lang_url = locales[lang] + window.location.pathname.slice(1,window.location.pathname.length);
      $.cookie("is_lang_detected",true ,{ path: '/'}); 
      window.location = locale_lang_url;
    } 
  }
        
  // --------------------- end of set language based on browser language -----------------
  </script>

	
	<link type="text/css" rel="stylesheet" href="/static/css/guide.css" />


  
<link rel="stylesheet" type="text/css" href="styles.css" />

</head>
  <body >
  	<!-- Google Tag Manager -->
    <script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5"
                              height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-58RLH5');</script>
    <!-- End Google Tag Manager -->
    
    <!-- new ga for staging server -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12395217-16"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-12395217-16');
    </script>
    <!-- new ga for staging server -->

    <!-- Elastic APM -->
    <script src="/static/js/elastic-apm-js-base.umd.min.1.0.0.js"></script>

    
    <script type="text/javascript">
      var active;
      
      active = Math.random() < 1
      
      elasticApm.init({
      active: active,
      serviceName: "elastic-co-frontend",
      serverUrl: "https://elastic-co-apm-server.app.elstc.co"
      })
      elasticApm.setTags({env:"production"});
      elasticApm.setInitialPageLoadName(window.location.href);
    </script>
    
    <!-- Elastic APM -->

    <!-- Header Section -->
    
<header class="header-wrapper">
  



  




<div class="navigation-wrapper">
  <div class="container">
    <nav class="navbar">
    <!-- Brand and toggle get grouped for better mobile display -->

    <div class="navbar-mobile-header navbar-header mr-20 mr-xs-0">
      <div class="row hidden-sm hidden-md hidden-lg">
        <div class="col-xs-5 hidden-sm hidden-md hidden-lg">
          <div class="navbar-mobile-header-icon navbar-mobile-header-icon-out">
            <span></span>
            <span></span>
            <span></span>
          </div>
          
        </div>
        <div class="col-xs-2 text-center">
          <div class="navbar-brand" id="elastic">
            <div id="logo">
              <a class="logo-m hidden-sm hidden-md hidden-lg" href="/">
                <img src="/assets/blta09706b1c5cc1e99/elastic-logo-mobile.svg" alt="elastic-logo-mobile">
              </a>
            </div>
          </div>
        </div>
        <div class="col-xs-5 hidden-sm hidden-md hidden-lg text-right">
          <ul class="menu-m hidden-sm hidden-md hidden-lg">
            <li class=""><a class="icon-contact" href="/contact"></a></li>

            <li id="searchbar"><a class="button icon" href="#" alt="Search"></a></li>
            <li class="lang global-language">
                
                  
                
                  
                    <a href="#">EN</a>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              <ul class="language-list">
                
                  
                    
                  
                  
                  <li><a href="/de/" data-value="/de/" id="locale-select">Deutsch</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="#" data-value="/" id="locale-select">English</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="/es/" data-value="/es/" id="locale-select">Español</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="/fr/" data-value="/fr/" id="locale-select">Français</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="/jp/" data-value="/jp/" id="locale-select">日本語</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="/kr/" data-value="/kr/" id="locale-select">한국어</a></li>
                  
                
                  
                    
                  
                  
                  <li><a href="/cn/" data-value="/cn/" id="locale-select">简体中文</a></li>
                  
                
              </ul>
            </li>
          </ul>
        </div>
      </div>

      <div class="navbar-brand hidden-xs" id="elastic">
        <div id="logo">
          <a class="hidden-xs" id="elastic-logo" href="/">
            <img src="/assets/blt244a845f141977c3/elastic-logo.svg" alt="elastic-logo">
          </a>
        </div>
      </div>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse mobile-inner-nav">
      <nav class="nav navbar-nav primary-nav-hover" id="nav">
        <ul>
        
          
            
              
            
            <li>
              <a class=" " href="/products">Products</a>
            </li>
          
        
          
            
              
            
            <li>
              <a class=" " href="/cloud">Cloud</a>
            </li>
          
        
          
            
              
            
            <li>
              <a class=" " href="/services">Services</a>
            </li>
          
        
          
            
              
            
            <li>
              <a class=" " href="/use-cases">Customers</a>
            </li>
          
        
          
            
              
            
            <li>
              <a class=" " href="/learn">Learn</a>
            </li>
          
        
          
        
        </ul>
      </nav>
      <nav class="nav navbar-nav navbar-right hidden-xs">
        <ul>
          <li class="hidden-xs hidden-sm"><a href="/downloads">downloads</a></li>
          <li class="hidden-xs hidden-md hidden-lg"><a class="icon-download" href="/downloads"></a></li>
          <!-- To set parameters to contact button -->
          
           
            
          

          
            
            
          
          <li class="hidden-xs hidden-sm"><a class="btn btn-secondary btn-contact" href="/contact?storm=global-header-en">contact</a></li>
          <!-- ////////////////ends here///////////////-->          
          <li class="hidden-xs hidden-md hidden-lg"><a class="icon-contact" href="/contact?storm=global-header-en"></a></li>
          <li class="hidden-xs" id="searchbar"><a class="button icon" href="#" alt="Search"></a></li>

          <li class="hidden-xs lang global-language">
                
                  
                
                  
                    <a href="#">EN</a>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
               <ul class="language-list">
              
                
                  
                
              
              <li><a class="" href="/de/" data-value="/de/">Deutsch</a></li>
              
              
                
                  
                
              
              <li><a class="" href="#" data-value="/">English</a></li>
              
              
                
                  
                
              
              <li><a class="" href="/es/" data-value="/es/">Español</a></li>
              
              
                
                  
                
              
              <li><a class="" href="/fr/" data-value="/fr/">Français</a></li>
              
              
                
                  
                
              
              <li><a class="" href="/jp/" data-value="/jp/">日本語</a></li>
              
              
                
                  
                
              
              <li><a class="" href="/kr/" data-value="/kr/">한국어</a></li>
              
              
                
                  
                
              
              <li><a class="" href="/cn/" data-value="/cn/">简体中文</a></li>
              
              
            </ul>
          </li>
        </ul>
      </nav>
    </div><!-- /.navbar-collapse -->
  </nav>
  </div>
</div>

<div class="header-search-wrapper open-search"> 
  <div class="container">
    <div class="big-search">
      <i class="big-search-icon"></i>
      <form id="searchfrm" name="searchForm" autocomplete="on" action="/search" method="get">
        <ul class="tags-wrapper">
          <li class="search-field"><input type="text" name="q" id="autocomplete" class="form-control global-input" autocomplete="off"></li>
        </ul>
      </form>
      <a class="header-search-cancel" href="#"></a>
    </div>
    <div class="nav-auto-complete"></div>
  </div>
</div>
</header>
<link rel="stylesheet" type="text/css" href="/static/css/horizon-swiper.min.css">
<style> 
    
      .m-shortcuts li a.m-search {
        background: url("/assets/blt7b9c4fe6528dc4ef/m-search-icon.png") no-repeat scroll center center rgba(0, 0, 0, 0);
        background-size: contain;}
    
      .m-shortcuts li a.m-docs {
        background: url("/assets/blt44d8b7530a76d01c/m-guide-icon.png") no-repeat scroll center center rgba(0, 0, 0, 0);
        background-size: contain;}
    
      .m-shortcuts li a.m-contact {
        background: url("/assets/blt878040795c9d083b/m-contact-icon.png") no-repeat scroll center center rgba(0, 0, 0, 0);
        background-size: contain;}
    
    
</style>
<script type="text/javascript" src="/static/js/horizon-swiper.min.js"></script>
<script type="text/javascript">
  $(document).ready( function(){
    // if($('#scrollable-pane li').length > 3){
      $('.horizon-swiper').horizonSwiper({
        showItems: 3,
        arrows: true
      }); 
    // }
    
    $('.navbar-toggle').on('click', function(){
      $('.mobile-navigation, .menu-open').hide();
      $('.navigation-wrapper').addClass('mobile-navigation');
      $('.navbar').addClass('menu-open');
      $('.menu-close').show();
    });
    $('.menu-close').on('click', function(){
      $('.navigation-wrapper').removeClass('mobile-navigation');
      $('.navbar').removeClass('menu-open');
      $('.menu-close').hide();
    });

    var active_url = "";

    $(".navbar-mobile-header-icon").click(function(index){
      $('.navigation-wrapper').toggleClass('menu-overlay');
      $(this).toggleClass("navbar-mobile-header-icon-click navbar-mobile-header-icon-out");
      $(".mobile-inner-nav").slideToggle(500);
      $(".mobile-inner-nav .navbar-nav a").each(function( index ) {
        $(this).css({'animation-delay': (index/25)+'0.4s'});
      });
    });

    $('.dropdown-btn').on('click', function(e){
      e.preventDefault();
      setDropdown();
    });
    
    function setDropdown() {
      $('#myDropdown').slideToggle();
    }

    $('body').on('click', function(event){
      if (!event.target.matches('.dropdown-btn')) {

        var dropdowns = $(".dropdown-content");
        var i;
        for (i = 0; i < dropdowns.length; i++) {
          var openDropdown = dropdowns[i];
          if (openDropdown.classList.contains('show')) {
            openDropdown.classList.remove('show');
          }
        }
      }
    });

    // if($(window).width() < 769){
    //   var $item = $('.scrollable-panel li'), 
    //       visible = 3, //Set the number of items that will be visible
    //       index = 0, //Starting index
    //       endIndex = ( $item.length / visible ) - 1; //End index (NOTE:: Requires visible to be a factor of $item.length... You can improve this by rounding...)

    //   $('#right-arrow').click(function(){
    //     if(index < endIndex ){
    //       index++;
    //       $item.animate({'left':'-=300px'});
    //     }
    //   });

    //   $('#left-arrow').click(function(){
    //     if(index > 0){
    //       index--;            
    //       $item.animate({'left':'+=300px'});
    //     }
    //   });
    // }
  });
  $(window).resize(function () {
    $('.horizon-swiper').horizonSwiper({
      showItems: 3,
      arrows: true
    });
  });
</script>
    
      


  





  
  
  
  
    <div class="tertiary-nav bdr-btm-e0e0e0">
      <div class="container">
        <div class="p-t-b-15 clearfix">
          <div class="breadcrum-wrapper pull-left text-left">
          
            <a href="/guide">Docs</a> 
          
          </div>
          
          
          
        </div>
      </div>
    </div>

    <div class="nav-mobile-dropdown hidden-sm hidden-md hidden-lg bdr-btm-e0e0e0 clearfix">
      <div class="container text-right">
        <div class="breadcrum-wrapper pull-left text-left">
          
              <a href="/guide">Docs</a>
          
        </div>
        
        
      </div>
    </div>
  


<style type="text/css">

  
</style>
    
    <div class="main-container">
      <section id="content" >
        <div class="content-wrapper">   
          
            
        			<!-- <div class="pageheader">
	<div class="common-container">
		
			<h2 class="page-title">Guide template</h2>
		
	</div>
</div> -->


        		
          

          
	<section id="guide" lang="en">
		<div class="container">
			<div class="row">
			  <!-- start body -->
				
            <div class="col-xs-12 col-sm-8 col-md-8 guide-section">
            <!-- start body -->
<div class="page_header">You are looking at documentation for an older release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div><div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch for Apache Hadoop
      [6.4]
    </a></span> » <span class="breadcrumb-link"><a href="reference.html">Elasticsearch for Apache Hadoop</a></span> » <span class="breadcrumb-node">Apache Spark support</span></div><div class="navheader"><span class="prev"><a href="pig.html">
              « 
              Apache Pig support</a>
           
        </span><span class="next">
           
          <a href="storm.html">Apache Storm support
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="spark"></a>Apache Spark support<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org" target="_top">Apache Spark</a> is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark website
</span></td></tr></table></div><p>Spark provides fast iterative/functional-like capabilities over large data sets, typically by <span class="emphasis"><em>caching</em></span> data in memory. As opposed to the rest of the libraries mentioned in this documentation, Apache Spark is computing framework that is not tied to Map/Reduce itself however it does integrate with Hadoop, mainly to HDFS.
elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0</p><h3><a id="spark-installation"></a>Installation<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>Just like other libraries, elasticsearch-hadoop needs to be available in Spark’s classpath. As Spark has multiple deployment modes, this can translate to the target classpath, whether it is on only one node (as is the case with the local mode - which will be used through-out the documentation) or per-node depending on the desired infrastructure.</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="spark-native"></a>Native RDD support<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 2.1. </p></div></div><p>elasticsearch-hadoop provides <span class="emphasis"><em>native</em></span> integration between Elasticsearch and Apache Spark, in the form of an <code class="literal">RDD</code> (Resilient Distributed Dataset) (or <span class="emphasis"><em>Pair</em></span> <code class="literal">RDD</code> to be precise) that can read data from Elasticsearch. The <code class="literal">RDD</code> is offered in two <span class="emphasis"><em>flavors</em></span>: one for Scala (which returns the data as <code class="literal">Tuple2</code> with Scala collections) and one for Java (which returns the data as <code class="literal">Tuple2</code> containing <code class="literal">java.util</code> collections).</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Whenever possible, consider using the <span class="emphasis"><em>native</em></span> integration as it offers the best performance and maximum flexibility.</p></div></div><h4><a id="spark-native-cfg"></a>Configuration<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>To configure elasticsearch-hadoop for Apache Spark, one can set the various properties described in the <a class="xref" href="configuration.html" title="Configuration"><em>Configuration</em></a> chapter in the <a class="ulink" href="http://spark.apache.org/docs/1.6.2/programming-guide.html#initializing-spark" target="_top"><code class="literal">SparkConf</code></a> object:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName(appName).setMaster(master)
conf.set("es.index.auto.create", "true")</pre></div><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
conf.set("es.index.auto.create", "true");</pre></div><p><strong>Command-line. </strong>For those that want to set the properties through the command-line (either directly or by loading them from a file), note that Spark <span class="emphasis"><em>only</em></span> accepts those that start with the "spark." prefix and will <span class="emphasis"><em>ignore</em></span> the rest (and depending on the version a warning might be thrown). To work around this limitation, define the elasticsearch-hadoop properties by appending the <code class="literal">spark.</code> prefix (thus they become <code class="literal">spark.es.</code>) and elasticsearch-hadoop will automatically resolve them:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-bash">$ ./bin/spark-submit --conf spark.es.resource<a id="CO43-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>=index/type ...</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO43-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Notice the <code class="literal">es.resource</code> property which became <code class="literal">spark.es.resource</code>
</p></td></tr></table></div><h4><a id="spark-write"></a>Writing data to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>With elasticsearch-hadoop, any <code class="literal">RDD</code> can be saved to Elasticsearch as long as its content can be translated into documents. In practice this means the <code class="literal">RDD</code> type needs to be a <code class="literal">Map</code> (whether a Scala or a Java one), a <a class="ulink" href="http://docs.oracle.com/javase/tutorial/javabeans/" target="_top"><code class="literal">JavaBean</code></a> or a Scala <a class="ulink" href="http://docs.scala-lang.org/tutorials/tour/case-classes.html" target="_top">case class</a>. When that is not the case, one can easily <span class="emphasis"><em>transform</em></span> the data
in Spark or plug-in their own custom <a class="link" href="configuration.html#configuration-serialization" title="Serializationedit"><code class="literal">ValueWriter</code></a>.</p><h5><a id="spark-write-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>When using Scala, simply import the <code class="literal">org.elasticsearch.spark</code> package which, through the <a class="ulink" href="http://www.artima.com/weblogs/viewpost.jsp?thread=179766" target="_top"><span class="emphasis"><em>pimp my library</em></span></a> pattern, enriches the  <span class="emphasis"><em>any</em></span> <code class="literal">RDD</code> API with <code class="literal">saveToEs</code> methods:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext    <a id="CO44-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <a id="CO44-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

val conf = ...
val sc = new SparkContext(conf)         <a id="CO44-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)
val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")

sc.makeRDD<a id="CO44-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>(Seq(numbers, airports)).saveToEs<a id="CO44-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>("spark/docs")</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Scala API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">makeRDD</code> creates an ad-hoc <code class="literal">RDD</code> based on the collection specified; any other <code class="literal">RDD</code> (in Java or Scala) can be passed in
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the content (namely the two <span class="emphasis"><em>documents</em></span> (numbers and airports)) in Elasticsearch under <code class="literal">spark/docs</code>
</p></td></tr></table></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Scala users might be tempted to use <code class="literal">Seq</code> and the <code class="literal">→</code> notation for declaring <span class="emphasis"><em>root</em></span> objects (that is the JSON document) instead of using a <code class="literal">Map</code>. While similar, the first notation results in slightly different types that cannot be matched to a JSON document: <code class="literal">Seq</code> is an order sequence (in other words a list) while <code class="literal">→</code> creates a <code class="literal">Tuple</code> which is more or less an ordered, fixed number of elements. As such, a list of lists cannot be used as a document since it cannot be mapped to a JSON object; however it can be used freely within one. Hence why in the example above <code class="literal">Map(k→v)</code> was used instead of <code class="literal">Seq(k→v)</code></p></div></div><p>As an alternative to the <span class="emphasis"><em>implicit</em></span> import above, one can use elasticsearch-hadoop Spark support in Scala through <code class="literal">EsSpark</code> in the <code class="literal">org.elasticsearch.spark.rdd</code> package which acts as a utility class allowing explicit method invocations. Additionally instead of <code class="literal">Map</code>s (which are convenient but require one mapping per instance due to their difference in structure), use a <span class="emphasis"><em>case class</em></span> :</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext
import org.elasticsearch.spark.rdd.EsSpark                        <a id="CO45-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

// define a case class
case class Trip(departure: String, arrival: String)               <a id="CO45-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

val upcomingTrip = Trip("OTP", "SFO")
val lastWeekTrip = Trip("MUC", "OTP")

val rdd = sc.makeRDD(Seq(upcomingTrip, lastWeekTrip))             <a id="CO45-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
EsSpark.saveToEs(rdd, "spark/docs")                               <a id="CO45-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">EsSpark</code> import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Define a case class named <code class="literal">Trip</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create an <code class="literal">RDD</code> around the <code class="literal">Trip</code> instances
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Index the <code class="literal">RDD</code> explicitly through <code class="literal">EsSpark</code>
</p></td></tr></table></div><p>For cases where the id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) of the document needs to be specified, one can do so by setting the appropriate <a class="link" href="configuration.html#cfg-mapping" title="Mapping (when writing to Elasticsearch)edit">mapping</a> namely <code class="literal">es.mapping.id</code>. Following the previous example, to indicate to Elasticsearch to use the field <code class="literal">id</code> as the document id, update the <code class="literal">RDD</code> configuration (it is also possible to set the property on the <code class="literal">SparkConf</code> though due to its global effect it is discouraged):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">EsSpark.saveToEs(rdd, "spark/docs", Map("es.mapping.id" -&gt; "id"))</pre></div><h5><a id="spark-write-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Java users have a dedicated class that provides a similar functionality to <code class="literal">EsSpark</code>, namely <code class="literal">JavaEsSpark</code> in the <code class="literal">org.elasticsearch.spark.rdd.api.java</code> (a package similar to Spark’s <a class="ulink" href="https://spark.apache.org/docs/1.0.1/api/java/index.html?org/apache/spark/api/java/package-summary.html" target="_top">Java API</a>):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.api.java.JavaSparkContext;                              <a id="CO46-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;

import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;                        <a id="CO46-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);                              <a id="CO46-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

Map&lt;String, ?&gt; numbers = ImmutableMap.of("one", 1, "two", 2);                   <a id="CO46-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
Map&lt;String, ?&gt; airports = ImmutableMap.of("OTP", "Otopeni", "SFO", "San Fran");

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD = jsc.parallelize(ImmutableList.of(numbers, airports));<a id="CO46-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
JavaEsSpark.saveToEs(javaRDD, "spark/docs");                                    <a id="CO46-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Java API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
to simplify the example, use <a class="ulink" href="https://code.google.com/p/guava-libraries/" target="_top">Guava</a>(a dependency of Spark) <code class="literal">Immutable</code>* methods for simple <code class="literal">Map</code>, <code class="literal">List</code> creation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a simple <code class="literal">RDD</code> over the two collections; any other <code class="literal">RDD</code> (in Java or Scala) can be passed in
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the content (namely the two <span class="emphasis"><em>documents</em></span> (numbers and airports)) in Elasticsearch under <code class="literal">spark/docs</code>
</p></td></tr></table></div><p>The code can be further simplified by using Java 5 <span class="emphasis"><em>static</em></span> imports. Additionally, the <code class="literal">Map</code> (who’s mapping is dynamic due to its <span class="emphasis"><em>loose</em></span> structure) can be replaced with a <code class="literal">JavaBean</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">public class TripBean implements Serializable {
   private String departure, arrival;

   public TripBean(String departure, String arrival) {
       setDeparture(departure);
       setArrival(arrival);
   }

   public TripBean() {}

   public String getDeparture() { return departure; }
   public String getArrival() { return arrival; }
   public void setDeparture(String dep) { departure = dep; }
   public void setArrival(String arr) { arrival = arr; }
}</pre></div><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.rdd.api.java.JavaEsSpark;                <a id="CO47-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
...

TripBean upcoming = new TripBean("OTP", "SFO");
TripBean lastWeek = new TripBean("MUC", "OTP");

JavaRDD&lt;TripBean&gt; javaRDD = jsc.parallelize(
                            ImmutableList.of(upcoming, lastWeek));        <a id="CO47-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
saveToEs(javaRDD, "spark/docs");                                          <a id="CO47-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSpark</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
define an <code class="literal">RDD</code> containing <code class="literal">TripBean</code> instances (<code class="literal">TripBean</code> is a <code class="literal">JavaBean</code>)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
call <code class="literal">saveToEs</code> method without having to type <code class="literal">JavaEsSpark</code> again
</p></td></tr></table></div><p>Setting the document id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) is similar to its Scala counterpart, though potentially a bit more verbose depending on whether you are using the JDK classes or some other utilities (like Guava):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">JavaEsSpark.saveToEs(javaRDD, "spark/docs", ImmutableMap.of("es.mapping.id", "id"));</pre></div><h4><a id="spark-write-json"></a>Writing existing JSON to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases where the data in the <code class="literal">RDD</code> is already in JSON, elasticsearch-hadoop allows direct indexing <span class="emphasis"><em>without</em></span> applying any transformation; the data is taken as is and sent directly to Elasticsearch. As such, in this case, elasticsearch-hadoop expects either an <code class="literal">RDD</code>
containing <code class="literal">String</code> or byte arrays (<code class="literal">byte[]</code>/<code class="literal">Array[Byte]</code>), assuming each entry represents a JSON document. If the <code class="literal">RDD</code> does not have the proper signature, the <code class="literal">saveJsonToEs</code> methods cannot be applied (in Scala they will not be available).</p><h5><a id="spark-write-json-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val json1 = """{"reason" : "business", "airport" : "SFO"}"""      <a id="CO48-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
val json2 = """{"participants" : 5, "airport" : "OTP"}"""

new SparkContext(conf).makeRDD(Seq(json1, json2))
                      .saveJsonToEs("spark/json-trips") <a id="CO48-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">RDD</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation, it should not contains breakline character like \n or \r\n
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr></table></div><h5><a id="spark-write-json-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">String json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}";  <a id="CO49-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
String json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}";

JavaSparkContext jsc = ...
JavaRDD&lt;String&gt;<a id="CO49-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span> stringRDD = jsc.parallelize(ImmutableList.of(json1, json2));
JavaEsSpark.saveJsonToEs(stringRDD, "spark/json-trips");             <a id="CO49-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">RDD</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation, it should not contains breakline character like \n or \r\n
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
notice the <code class="literal">RDD&lt;String&gt;</code> signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr></table></div><h4><a id="spark-write-dyn"></a>Writing to dynamic/multi-resources<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases when the data being written to Elasticsearch needs to be indexed under different buckets (based on the data content) one can use the <code class="literal">es.resource.write</code> field which accepts a pattern that is resolved from the document content, at runtime. Following the aforementioned <a class="link" href="configuration.html#cfg-multi-writes" title="Dynamic/multi resource writesedit">media example</a>, one could configure it as follows:</p><h5><a id="spark-write-dyn-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val game = Map("media_type"<a id="CO50-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>-&gt;"game","title" -&gt; "FF VI","year" -&gt; "1994")
val book = Map("media_type" -&gt; "book","title" -&gt; "Harry Potter","year" -&gt; "2010")
val cd = Map("media_type" -&gt; "music","title" -&gt; "Surfing With The Alien")

sc.makeRDD(Seq(game, book, cd)).saveToEs("my-collection-{media_type}/doc")  <a id="CO50-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Document <span class="emphasis"><em>key</em></span> used for splitting the data. Any field can be declared (but make sure it is available in all documents)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, in this example based on <code class="literal">media_type</code>
</p></td></tr></table></div><p>For each document/object about to be written, elasticsearch-hadoop will extract the <code class="literal">media_type</code> field and use its value to determine the target resource.</p><h5><a id="spark-write-dyn-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>As expected, things in Java are strikingly similar:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">Map&lt;String, ?&gt; game =
  ImmutableMap.of("media_type", "game", "title", "FF VI", "year", "1994");
Map&lt;String, ?&gt; book = ...
Map&lt;String, ?&gt; cd = ...

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD =
                jsc.parallelize(ImmutableList.of(game, book, cd));
saveToEs(javaRDD, "my-collection-{media_type}/doc");  <a id="CO51-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO51-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, <code class="literal">media_type</code> in this example
</p></td></tr></table></div><h4><a id="spark-write-meta"></a>Handling document metadata<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Elasticsearch allows each document to have its own <a class="ulink" href="http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html" target="_top">metadata</a>. As explained above, through the various <a class="link" href="configuration.html#cfg-mapping" title="Mapping (when writing to Elasticsearch)edit">mapping</a> options one can customize these parameters so that their values are extracted from their belonging document. Further more, one can even include/exclude what parts of the data are sent back to Elasticsearch. In Spark, elasticsearch-hadoop extends this functionality allowing metadata to be supplied <span class="emphasis"><em>outside</em></span> the document itself through the use of <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs" target="_top"><span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>s</a>.
In other words, for <code class="literal">RDD</code>s containing a key-value tuple, the metadata can be extracted from the key and the value used as the document source.</p><p>The metadata is described through the <code class="literal">Metadata</code> Java <a class="ulink" href="http://docs.oracle.com/javase/tutorial/java/javaOO/enum.html" target="_top">enum</a> within <code class="literal">org.elasticsearch.spark.rdd</code> package which identifies its type - <code class="literal">id</code>, <code class="literal">ttl</code>, <code class="literal">version</code>, etc…
Thus an <code class="literal">RDD</code> keys can be a <code class="literal">Map</code> containing the <code class="literal">Metadata</code> for each document and its associated values. If <code class="literal">RDD</code> key is not of type <code class="literal">Map</code>, elasticsearch-hadoop will consider the object as representing the document id and use it accordingly.
This sounds more complicated than it is, so let us see some examples.</p><h5><a id="spark-write-meta-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Pair <code class="literal">RDD</code>s, or simply put <code class="literal">RDD</code>s with the signature <code class="literal">RDD[(K,V)]</code> can take advantage of the <code class="literal">saveToEsWithMeta</code> methods that are available either through the <span class="emphasis"><em>implicit</em></span> import of <code class="literal">org.elasticsearch.spark</code> package or <code class="literal">EsSpark</code> object.
To manually specify the id for each document, simply pass in the <code class="literal">Object</code> (not of type <code class="literal">Map</code>) in your <code class="literal">RDD</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// instance of SparkContext
val sc = ...

val airportsRDD<a id="CO52-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span> = sc.makeRDD(Seq((1, otp), (2, muc), (3, sfo)))  <a id="CO52-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
airportsRDD.saveToEsWithMeta<a id="CO52-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>("airports/2015")</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">airportsRDD</code> is a <span class="emphasis"><em>key-value</em></span> pair <code class="literal">RDD</code>; it is created from a <code class="literal">Seq</code> of <code class="literal">tuple</code>s
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The key of each tuple within the <code class="literal">Seq</code> represents the <span class="emphasis"><em>id</em></span> of its associated value/document; in other words, document <code class="literal">otp</code> has id <code class="literal">1</code>, <code class="literal">muc</code> <code class="literal">2</code> and <code class="literal">sfo</code> <code class="literal">3</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Since <code class="literal">airportsRDD</code> is a pair <code class="literal">RDD</code>, it has the <code class="literal">saveToEsWithMeta</code> method available. This tells elasticsearch-hadoop to pay special attention to the <code class="literal">RDD</code> keys and use them as metadata, in this case as document ids. If <code class="literal">saveToEs</code> would have been used instead, then elasticsearch-hadoop would consider the <code class="literal">RDD</code> tuple, that is both the key and the value, as part of the document.
</p></td></tr></table></div><p>When more than just the id needs to be specified, one should use a <code class="literal">scala.collection.Map</code> with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.elasticsearch.spark.rdd.Metadata._          <a id="CO53-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// metadata for each document
// note it's not required for them to have the same structure
val otpMeta = Map(ID -&gt; 1, TTL -&gt; "3h")                <a id="CO53-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
val mucMeta = Map(ID -&gt; 2, VERSION -&gt; "23")            <a id="CO53-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val sfoMeta = Map(ID -&gt; 3)                             <a id="CO53-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

// instance of SparkContext
val sc = ...

val airportsRDD = sc.makeRDD<a id="CO53-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>(Seq((otpMeta, otp), (mucMeta, muc), (sfoMeta, sfo)))
airportsRDD.saveToEsWithMeta("airports/2015") <a id="CO53-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Import the <code class="literal">Metadata</code> enum
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">otp</code> document. In this case, <code class="literal">ID</code> with a value of 1 and <code class="literal">TTL</code> with a value of <code class="literal">3h</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">muc</code> document. In this case, <code class="literal">ID</code> with a value of 2 and <code class="literal">VERSION</code> with a value of <code class="literal">23</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">sfo</code> document. In this case, <code class="literal">ID</code> with a value of 3
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata and the documents are assembled into a <span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">RDD</code> is saved accordingly using the <code class="literal">saveToEsWithMeta</code> method
</p></td></tr></table></div><h5><a id="spark-write-meta-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In a similar fashion, on the Java side, <code class="literal">JavaEsSpark</code> provides <code class="literal">saveToEsWithMeta</code> methods that are applied to <code class="literal">JavaPairRDD</code> (the equivalent in Java of <code class="literal">RDD[(K,V)]</code>). Thus to save documents based on their ids one can use:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; jfk = ImmutableMap.of("iata", "JFK", "name", "JFK NYC");

JavaSparkContext jsc = ...

// create a pair RDD between the id and the docs
JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs<a id="CO54-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(1, otp),          <a id="CO54-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(2, jfk)));        <a id="CO54-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <a id="CO54-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a <code class="literal">JavaPairRDD</code> by using Scala <code class="literal">Tuple2</code> class wrapped around the document id and the document itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the first document wrapped around the id (<code class="literal">1</code>) and the doc (<code class="literal">otp</code>) itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the second document wrapped around the id (<code class="literal">2</code>) and <code class="literal">jfk</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">JavaPairRDD</code> is saved accordingly using the keys as a id and the values as documents
</p></td></tr></table></div><p>When more than just the id needs to be specified, one can choose to use a <code class="literal">java.util.Map</code> populated with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;
import org.elasticsearch.spark.rdd.Metadata;          <a id="CO55-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

import static org.elasticsearch.spark.rdd.Metadata.*; <a id="CO55-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; sfo = ImmutableMap.of("iata", "SFO", "name", "San Fran");

// metadata for each document
// note it's not required for them to have the same structure
Map&lt;Metadata, Object&gt; otpMeta<a id="CO55-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt;<a id="CO55-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span> of(ID, 1, TTL, "1d");
Map&lt;Metadata, Object&gt; sfoMeta<a id="CO55-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt; of(ID, "2", VERSION, "23");

JavaSparkContext jsc = ...

// create a pair RDD between the id and the docs
JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs&lt;(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(otpMeta, otp),    <a id="CO55-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(sfoMeta, sfo)));  <a id="CO55-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <a id="CO55-8"></a><span><img src="images/icons/callouts/8.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">Metadata</code> <code class="literal">enum</code> describing the document metadata that can be declared
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
static import for the <code class="literal">enum</code> to refer to its values in short format (<code class="literal">ID</code>, <code class="literal">TTL</code>, etc…)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">otp</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Boiler-plate construct for forcing the <code class="literal">of</code> method generic signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">sfo</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple between <code class="literal">otp</code> (as the value) and its metadata (as the key)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple associating <code class="literal">sfo</code> and its metadata
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-8"><span><img src="images/icons/callouts/8.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">saveToEsWithMeta</code> invoked over the <code class="literal">JavaPairRDD</code> containing documents and their respective metadata
</p></td></tr></table></div><h4><a id="spark-read"></a>Reading data from Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For reading, one should define the Elasticsearch <code class="literal">RDD</code> that <span class="emphasis"><em>streams</em></span> data from Elasticsearch to Spark.</p><h5><a id="spark-read-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Similar to writing, the <code class="literal">org.elasticsearch.spark</code> package, enriches the <code class="literal">SparkContext</code> API with <code class="literal">esRDD</code> methods:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext    <a id="CO56-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <a id="CO56-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

val conf = ...
val sc = new SparkContext(conf)         <a id="CO56-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

val RDD = sc.esRDD("radio/artists")     <a id="CO56-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Scala API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
a dedicated <code class="literal">RDD</code> for Elasticsearch is created for index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>The method can be overloaded to specify an additional query or even a configuration <code class="literal">Map</code> (overriding <code class="literal">SparkConf</code>):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">...
import org.elasticsearch.spark._

...
val conf = ...
val sc = new SparkContext(conf)

sc.esRDD("radio/artists", "?q=me*") <a id="CO57-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create an <code class="literal">RDD</code> streaming all the documents matching <code class="literal">me*</code> from index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>The documents from Elasticsearch are returned, by default, as a <code class="literal">Tuple2</code> containing as the first element the document id and the second element the actual document represented through Scala <a class="ulink" href="http://docs.scala-lang.org/overviews/collections/overview.html" target="_top">collections</a>, namely one `Map[String, Any]`where the keys represent the field names and the value their respective values.</p><h5><a id="spark-read-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Java users have a dedicated <code class="literal">JavaPairRDD</code> that works the same as its Scala counterpart however the returned <code class="literal">Tuple2</code> values (or second element) returns the documents as native, <code class="literal">java.util</code> collections.</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.api.java.JavaSparkContext;               <a id="CO58-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;             <a id="CO58-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);               <a id="CO58-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD&lt;String, Map&lt;String, Object&gt;&gt; esRDD =
                        JavaEsSpark.esRDD(jsc, "radio/artists"); <a id="CO58-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Java API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
a dedicated <code class="literal">JavaPairRDD</code> for Elasticsearch is created for index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>In a similar fashion one can use the overloaded <code class="literal">esRDD</code> methods to specify a query or pass a <code class="literal">Map</code> object for advanced configuration.
Let us see how this looks, but this time around using <a class="ulink" href="http://docs.oracle.com/javase/1.5.0/docs/guide/language/static-import.html" target="_top">Java static imports</a>. Further more, let us discard the documents ids and retrieve only the <code class="literal">RDD</code> values:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.rdd.api.java.JavaEsSpark.*;   <a id="CO59-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

...
JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?q=me*"<a id="CO59-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>).values()<a id="CO59-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>;</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSpark</code> class
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create an <code class="literal">RDD</code> streaming all the documents starting with <code class="literal">me</code> from index <code class="literal">radio/artists</code>. Note the method does not have to be fully qualified due to the static import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
return only <span class="emphasis"><em>values</em></span> of the <code class="literal">PairRDD</code> - hence why the result is of type <code class="literal">JavaRDD</code> and <span class="emphasis"><em>not</em></span> <code class="literal">JavaPairRDD</code>
</p></td></tr></table></div><p>By using the <code class="literal">JavaEsSpark</code> API, one gets a hold of Spark’s dedicated <code class="literal">JavaPairRDD</code> which are better suited in Java environments than the base <code class="literal">RDD</code> (due to its Scala
signatures). Moreover, the dedicated <code class="literal">RDD</code> returns Elasticsearch documents as proper Java collections so one does not have to deal with Scala collections (which
is typically the case with <code class="literal">RDD</code>s). This is particularly powerful when using Java 8, which we strongly recommend as its
<a class="ulink" href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html" target="_top">lambda expressions</a> make collection processing <span class="emphasis"><em>extremely</em></span> concise.</p><p>To wit, let us assume one wants to filter the documents from the <code class="literal">RDD</code> and return only those that contain a value that contains <code class="literal">mega</code> (please ignore the fact one can and should do the filtering directly through Elasticsearch).</p><p>In versions prior to Java 8, the code would look something like this:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?q=me*").values();
JavaRDD&lt;Map&lt;String, Object&gt;&gt; filtered = esRDD.filter(
    new Function&lt;Map&lt;String, Object&gt;, Boolean&gt;() {
      @Override
      public Boolean call(Map&lt;String, Object&gt; map) throws Exception {
          returns map.contains("mega");
      }
    });</pre></div><p>with Java 8, the filtering becomes a one liner:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?q=me*").values();
JavaRDD&lt;Map&lt;String, Object&gt;&gt; filtered = esRDD.filter(doc -&gt;
                                                doc.contains("mega"));</pre></div><h5><a id="spark-read-json"></a>Reading data in JSON format<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In case where the results from Elasticsearch need to be in JSON format (typically to be sent down the wire to some other system), one can use the dedicated <code class="literal">esJsonRDD</code> methods. In this case, the connector will return the documents content as it is received from Elasticsearch without any processing as an <code class="literal">RDD[(String, String)]</code> in Scala or <code class="literal">JavaPairRDD[String, String]</code> in Java with the keys representing the document id and the value its actual content in JSON format.</p><h4><a id="spark-type-conversion"></a>Type conversion<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>When dealing with multi-value/array fields, please see <a class="link" href="mapping.html#mapping-multi-values" title="Handling array/multi-values fieldsedit">this</a> section and in particular <a class="link" href="configuration.html#cfg-field-info" title="Field information (when reading from Elasticsearch)edit">these</a> configuration options.
IMPORTANT: If automatic index creation is used, please review <a class="link" href="mapping.html#auto-mapping-type-loss">this</a> section for more information.</p></div></div><p>elasticsearch-hadoop automatically converts Spark built-in types to Elasticsearch <a class="ulink" href="http://www.elastic.co/guide/en/elasticsearch/reference/5.0/mapping-types.html" target="_top">types</a> (and back) as shown in the table below:</p><div class="table"><a id="id-1.3.16.6.97"></a><p class="title"><strong>Table 5. Scala Types Conversion Table</strong></p><div class="table-contents"><table summary="Scala Types Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Scala type </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">None</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Unit</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Nil</code></p></td><td align="center" valign="top"><p>empty <code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Some[T]</code></p></td><td align="center" valign="top"><p><code class="literal">T</code> according to the table</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Map</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Traversable</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><span class="emphasis"><em>case class</em></span></p></td><td align="center" valign="top"><p><code class="literal">object</code> (see <code class="literal">Map</code>)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Product</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr></tbody></table></div></div><br class="table-break" /><p>in addition, the following <span class="emphasis"><em>implied</em></span> conversion applies for Java types:</p><div class="table"><a id="id-1.3.16.6.99"></a><p class="title"><strong>Table 6. Java Types Conversion Table</strong></p><div class="table-contents"><table summary="Java Types Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Java type </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">null</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">String</code></p></td><td align="center" valign="top"><p><code class="literal">string</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Boolean</code></p></td><td align="center" valign="top"><p><code class="literal">boolean</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Byte</code></p></td><td align="center" valign="top"><p><code class="literal">byte</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Short</code></p></td><td align="center" valign="top"><p><code class="literal">short</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Integer</code></p></td><td align="center" valign="top"><p><code class="literal">int</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Long</code></p></td><td align="center" valign="top"><p><code class="literal">long</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Double</code></p></td><td align="center" valign="top"><p><code class="literal">double</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Float</code></p></td><td align="center" valign="top"><p><code class="literal">float</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Number</code></p></td><td align="center" valign="top"><p><code class="literal">float</code> or <code class="literal">double</code> (depending on size)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Calendar</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Date</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Timestamp</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">byte[]</code></p></td><td align="center" valign="top"><p><code class="literal">string</code> (BASE64)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Object[]</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Iterable</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Map</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><span class="emphasis"><em>Java Bean</em></span></p></td><td align="center" valign="top"><p><code class="literal">object</code> (see <code class="literal">Map</code>)</p></td></tr></tbody></table></div></div><br class="table-break" /><p>The conversion is done as a <span class="emphasis"><em>best</em></span> effort; built-in Java and Scala types are guaranteed to be properly converted, however there are no guarantees for user types whether in Java or Scala. As mentioned in the tables above, when a <code class="literal">case</code> class is encountered in Scala or <code class="literal">JavaBean</code> in Java, the converters will try to <code class="literal">unwrap</code> its content and save it as an <code class="literal">object</code>. Note this works only for top-level user objects - if the user object has other user objects nested in, the conversion is likely to fail since the converter does not perform nested <code class="literal">unwrapping</code>.
This is done on purpose since the converter has to <span class="emphasis"><em>serialize</em></span> and <span class="emphasis"><em>deserialize</em></span> the data and user types introduce ambiguity due to data loss; this can be addressed through some type of mapping however that takes the project way too close to the realm of ORMs and arguably introduces too much complexity for little to no gain; thanks to the processing functionality in Spark and the plugability in elasticsearch-hadoop one can easily transform objects into other types, if needed with minimal effort and maximum control.</p><p><strong>Geo types. </strong>It is worth mentioning that rich data types available only in Elasticsearch, such as <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-point.html" target="_top"><code class="literal">GeoPoint</code></a> or <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-shape.html" target="_top"><code class="literal">GeoShape</code></a> are supported by converting their structure into the primitives available in the table above.
For example, based on its storage a <code class="literal">geo_point</code> might be returned as a <code class="literal">String</code> or a <code class="literal">Traversable</code>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="spark-streaming"></a>Spark Streaming support<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 5.0. </p></div></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org/streaming/" target="_top">Spark Streaming</a> is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark website
</span></td></tr></table></div><p>Spark Streaming is an extension on top of the core Spark functionality that allows near real time processing of stream data. Spark Streaming works around the idea of <code class="literal">DStream</code>s, or <span class="emphasis"><em>Discretized Streams</em></span>. <code class="literal">DStreams</code> operate by collecting newly arrived records into a small <code class="literal">RDD</code> and executing it. This repeats every few seconds with a new <code class="literal">RDD</code> in a process called <span class="emphasis"><em>microbatching</em></span>. The <code class="literal">DStream</code> api includes many of the same processing operations as the <code class="literal">RDD</code> api, plus a few other streaming specific methods. elasticsearch-hadoop provides native integration with Spark Streaming as of version 5.0.</p><p>When using the elasticsearch-hadoop Spark Streaming support, Elasticsearch can be targeted as an output location to index data into from a Spark Streaming job in the same way that one might persist the results from an <code class="literal">RDD</code>. Though, unlike <code class="literal">RDD</code>s, you are unable to read data out of Elasticsearch using a <code class="literal">DStream</code> due to the continuous nature of it.</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Spark Streaming support provides special optimizations to allow for conservation of network resources on Spark executors when running jobs with very small processing windows. For this reason, one should prefer to use this integration instead of invoking <code class="literal">saveToEs</code> on <code class="literal">RDD</code>s returned from the <code class="literal">foreachRDD</code> call on <code class="literal">DStream</code>.</p></div></div><h4><a id="spark-streaming-write"></a>Writing <code class="literal">DStream</code> to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Like <code class="literal">RDD</code>s, any <code class="literal">DStream</code> can be saved to Elasticsearch as long as its content can be translated into documents. In practice this means the <code class="literal">DStream</code> type needs to be a <code class="literal">Map</code> (either a Scala or a Java one), a <a class="ulink" href="http://docs.oracle.com/javase/tutorial/javabeans/" target="_top"><code class="literal">JavaBean</code></a> or a Scala <a class="ulink" href="http://docs.scala-lang.org/tutorials/tour/case-classes.html" target="_top">case class</a>. When that is not the case, one can easily <span class="emphasis"><em>transform</em></span> the data
in Spark or plug-in their own custom <a class="link" href="configuration.html#configuration-serialization" title="Serializationedit"><code class="literal">ValueWriter</code></a>.</p><h5><a id="spark-streaming-write-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>When using Scala, simply import the <code class="literal">org.elasticsearch.spark.streaming</code> package which, through the <a class="ulink" href="http://www.artima.com/weblogs/viewpost.jsp?thread=179766" target="_top"><span class="emphasis"><em>pimp my library</em></span></a> pattern, enriches the <code class="literal">DStream</code> API with <code class="literal">saveToEs</code> methods:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._               <a id="CO60-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext._

import org.elasticsearch.spark.streaming._           <a id="CO60-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

val conf = ...
val sc = new SparkContext(conf)                      <a id="CO60-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val ssc = new StreamingContext(sc, Seconds(1))       <a id="CO60-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)
val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")

val rdd = sc.makeRDD(Seq(numbers, airports))
val microbatches = mutable.Queue(rdd)                <a id="CO60-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>

ssc.queueStream(microbatches).saveToEs<a id="CO60-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>("spark/docs")

ssc.start()
ssc.awaitTermination() <a id="CO60-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark and Spark Streaming Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Spark Streaming imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Scala API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start SparkStreaming context by passing it the SparkContext. The microbatches will be processed every second.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">makeRDD</code> creates an ad-hoc <code class="literal">RDD</code> based on the collection specified; any other <code class="literal">RDD</code> (in Java or Scala) can be passed in. Create a queue of `RDD`s to signify the microbatches to perform.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a <code class="literal">DStream</code> out of the <code class="literal">RDD`s and index the content (namely the two _documents_ (numbers and airports)) in {es} under `spark/docs</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Start the spark Streaming Job and wait for it to eventually finish.
</p></td></tr></table></div><p>As an alternative to the <span class="emphasis"><em>implicit</em></span> import above, one can use elasticsearch-hadoop Spark Streaming support in Scala through <code class="literal">EsSparkStreaming</code> in the <code class="literal">org.elasticsearch.spark.streaming</code> package which acts as a utility class allowing explicit method invocations. Additionally instead of <code class="literal">Map</code>s (which are convenient but require one mapping per instance due to their difference in structure), use a <span class="emphasis"><em>case class</em></span> :</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext
import org.elasticsearch.spark.streaming.EsSparkStreaming         <a id="CO61-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

// define a case class
case class Trip(departure: String, arrival: String)               <a id="CO61-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

val upcomingTrip = Trip("OTP", "SFO")
val lastWeekTrip = Trip("MUC", "OTP")

val rdd = sc.makeRDD(Seq(upcomingTrip, lastWeekTrip))
val microbatches = mutable.Queue(rdd)                             <a id="CO61-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val dstream = ssc.queueStream(microbatches)

EsSparkStreaming.saveToEs(dstream, "spark/docs")                  <a id="CO61-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

ssc.start()                                                       <a id="CO61-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">EsSparkStreaming</code> import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Define a case class named <code class="literal">Trip</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a <code class="literal">DStream</code> around the <code class="literal">RDD</code> of <code class="literal">Trip</code> instances
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the <code class="literal">DStream</code> to be indexed explicitly through <code class="literal">EsSparkStreaming</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Start the streaming process
</p></td></tr></table></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Once a SparkStreamingContext is started, no new <code class="literal">DStream</code>s can be added or configured. Once a context has stopped, it cannot be restarted. There can only be one active SparkStreamingContext at a time per JVM. Also note that when stopping a SparkStreamingContext programmatically, it stops the underlying SparkContext unless instructed not to.</p></div></div><p>For cases where the id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) of the document needs to be specified, one can do so by setting the appropriate <a class="link" href="configuration.html#cfg-mapping" title="Mapping (when writing to Elasticsearch)edit">mapping</a> namely <code class="literal">es.mapping.id</code>. Following the previous example, to indicate to Elasticsearch to use the field <code class="literal">id</code> as the document id, update the <code class="literal">DStream</code> configuration (it is also possible to set the property on the <code class="literal">SparkConf</code> though due to its global effect it is discouraged):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">EsSparkStreaming.saveToEs(dstream, "spark/docs", Map("es.mapping.id" -&gt; "id"))</pre></div><h5><a id="spark-streaming-write-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Java users have a dedicated class that provides a similar functionality to <code class="literal">EsSparkStreaming</code>, namely <code class="literal">JavaEsSparkStreaming</code> in the package <code class="literal">org.elasticsearch.spark.streaming.api.java</code> (a package similar to Spark’s <a class="ulink" href="https://spark.apache.org/docs/1.6.1/api/java/index.html?org/apache/spark/streaming/api/java/package-summary.html" target="_top">Java API</a>):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;                                              <a id="CO62-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.api.java.JavaDStream;

import org.elasticsearch.spark.streaming.api.java.JavaEsSparkStreaming;         <a id="CO62-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);                              <a id="CO62-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
JavaStreamingContext jssc = new JavaSparkStreamingContext(jsc, Seconds.apply(1));

Map&lt;String, ?&gt; numbers = ImmutableMap.of("one", 1, "two", 2);                   <a id="CO62-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
Map&lt;String, ?&gt; airports = ImmutableMap.of("OTP", "Otopeni", "SFO", "San Fran");

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD = jsc.parallelize(ImmutableList.of(numbers, airports));
Queue&lt;JavaRDD&lt;Map&lt;String, ?&gt;&gt;&gt; microbatches = new LinkedList&lt;&gt;();
microbatches.add(javaRDD);                                                      <a id="CO62-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
JavaDStream&lt;Map&lt;String, ?&gt;&gt; javaDStream = jssc.queueStream(microbatches);

JavaEsSparkStreaming.saveToEs(javaDStream, "spark/docs");                       <a id="CO62-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>

jssc.start()                                                                    <a id="CO62-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark and Spark Streaming Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark and Spark Streaming through its Java API. The microbatches will be processed every second.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
to simplify the example, use <a class="ulink" href="https://code.google.com/p/guava-libraries/" target="_top">Guava</a>(a dependency of Spark) <code class="literal">Immutable</code>* methods for simple <code class="literal">Map</code>, <code class="literal">List</code> creation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a simple <code class="literal">DStream</code> over the microbatch; any other <code class="literal">RDD</code>s (in Java or Scala) can be passed in
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the content (namely the two <span class="emphasis"><em>documents</em></span> (numbers and airports)) in Elasticsearch under <code class="literal">spark/docs</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
execute the streaming job.
</p></td></tr></table></div><p>The code can be further simplified by using Java 5 <span class="emphasis"><em>static</em></span> imports. Additionally, the <code class="literal">Map</code> (who’s mapping is dynamic due to its <span class="emphasis"><em>loose</em></span> structure) can be replaced with a <code class="literal">JavaBean</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">public class TripBean implements Serializable {
   private String departure, arrival;

   public TripBean(String departure, String arrival) {
       setDeparture(departure);
       setArrival(arrival);
   }

   public TripBean() {}

   public String getDeparture() { return departure; }
   public String getArrival() { return arrival; }
   public void setDeparture(String dep) { departure = dep; }
   public void setArrival(String arr) { arrival = arr; }
}</pre></div><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.rdd.api.java.JavaEsSparkStreaming;  <a id="CO63-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
...

TripBean upcoming = new TripBean("OTP", "SFO");
TripBean lastWeek = new TripBean("MUC", "OTP");

JavaRDD&lt;TripBean&gt; javaRDD = jsc.parallelize(ImmutableList.of(upcoming, lastWeek));
Queue&lt;JavaRDD&lt;TripBean&gt;&gt; microbatches = new LinkedList&lt;JavaRDD&lt;TripBean&gt;&gt;();
microbatches.add(javaRDD);
JavaDStream&lt;TripBean&gt; javaDStream = jssc.queueStream(microbatches);       <a id="CO63-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

saveToEs(javaDStream, "spark/docs");                                          <a id="CO63-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

jssc.start()                                                              <a id="CO63-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSparkStreaming</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
define a <code class="literal">DStream</code> containing <code class="literal">TripBean</code> instances (<code class="literal">TripBean</code> is a <code class="literal">JavaBean</code>)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
call <code class="literal">saveToEs</code> method without having to type <code class="literal">JavaEsSparkStreaming</code> again
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
run that Streaming job
</p></td></tr></table></div><p>Setting the document id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) is similar to its Scala counterpart, though potentially a bit more verbose depending on whether you are using the JDK classes or some other utilities (like Guava):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">JavaEsSparkStreaming.saveToEs(javaDStream, "spark/docs", ImmutableMap.of("es.mapping.id", "id"));</pre></div><h4><a id="spark-streaming-write-json"></a>Writing Existing JSON to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases where the data being streamed by the <code class="literal">DStream</code> is already serialized as JSON, elasticsearch-hadoop allows direct indexing <span class="emphasis"><em>without</em></span> applying any transformation; the data is taken as is and sent directly to Elasticsearch. As such, in this case, elasticsearch-hadoop expects either a <code class="literal">DStream</code> containing <code class="literal">String</code> or byte arrays (<code class="literal">byte[]</code>/<code class="literal">Array[Byte]</code>), assuming each entry represents a JSON document. If the <code class="literal">DStream</code> does not have the proper signature, the <code class="literal">saveJsonToEs</code> methods cannot be applied (in Scala they will not be available).</p><h5><a id="spark-streaming-write-json-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val json1 = """{"reason" : "business", "airport" : "SFO"}"""      <a id="CO64-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
val json2 = """{"participants" : 5, "airport" : "OTP"}"""

val sc = new SparkContext(conf)
val ssc = new StreamingContext(sc, Seconds(1))

val rdd = sc.makeRDD(Seq(json1, json2))
val microbatch = mutable.Queue(rdd)
ssc.queueStream(microbatch).saveJsonToEs("spark/json-trips")      <a id="CO64-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

ssc.start()                                                       <a id="CO64-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">DStream</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
configure the stream to index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start the streaming job
</p></td></tr></table></div><h5><a id="spark-streaming-write-json-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">String json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}";  <a id="CO65-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
String json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}";

JavaSparkContext jsc = ...
JavaStreamingContext jssc = ...
JavaRDD&lt;String&gt; stringRDD = jsc.parallelize(ImmutableList.of(json1, json2));
Queue&lt;JavaRDD&lt;String&gt;&gt; microbatches = new LinkedList&lt;JavaRDD&lt;String&gt;&gt;();      <a id="CO65-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
microbatches.add(stringRDD);
JavaDStream&lt;String&gt;<a id="CO65-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span> stringDStream = jssc.queueStream(microbatches);

JavaEsSparkStreaming.saveJsonToEs(stringRDD, "spark/json-trips");    <a id="CO65-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

jssc.start()                                                         <a id="CO65-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">DStream</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
creating an <code class="literal">RDD</code>, placing it into a queue, and creating a <code class="literal">DStream</code> out of the queued <code class="literal">RDD</code>s, treating each as a microbatch.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
notice the <code class="literal">JavaDStream&lt;String&gt;</code> signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
configure stream to index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
launch stream job
</p></td></tr></table></div><h4><a id="spark-streaming-write-dyn"></a>Writing to dynamic/multi-resources<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases when the data being written to Elasticsearch needs to be indexed under different buckets (based on the data content) one can use the <code class="literal">es.resource.write</code> field which accepts a pattern that is resolved from the document content, at runtime. Following the aforementioned <a class="link" href="configuration.html#cfg-multi-writes" title="Dynamic/multi resource writesedit">media example</a>, one could configure it as follows:</p><h5><a id="spark-streaming-write-dyn-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val game = Map("media_type"<a id="CO66-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>-&gt;"game","title" -&gt; "FF VI","year" -&gt; "1994")
val book = Map("media_type" -&gt; "book","title" -&gt; "Harry Potter","year" -&gt; "2010")
val cd = Map("media_type" -&gt; "music","title" -&gt; "Surfing With The Alien")

val batch = sc.makeRDD(Seq(game, book, cd))
val microbatches = mutable.Queue(batch)
ssc.queueStream(microbatches).saveToEs("my-collection-{media_type}/doc")  <a id="CO66-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
ssc.start()</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO66-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Document <span class="emphasis"><em>key</em></span> used for splitting the data. Any field can be declared (but make sure it is available in all documents)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO66-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, in this example based on <code class="literal">media_type</code>
</p></td></tr></table></div><p>For each document/object about to be written, elasticsearch-hadoop will extract the <code class="literal">media_type</code> field and use its value to determine the target resource.</p><h5><a id="spark-streaming-write-dyn-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>As expected, things in Java are strikingly similar:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">Map&lt;String, ?&gt; game =
  ImmutableMap.of("media_type", "game", "title", "FF VI", "year", "1994");
Map&lt;String, ?&gt; book = ...
Map&lt;String, ?&gt; cd = ...

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD =
                jsc.parallelize(ImmutableList.of(game, book, cd));
Queue&lt;JavaRDD&lt;Map&lt;String, ?&gt;&gt;&gt; microbatches = ...
JavaDStream&lt;Map&lt;String, ?&gt;&gt; javaDStream =
                jssc.queueStream(microbatches);

saveToEs(javaDStream, "my-collection-{media_type}/doc");  <a id="CO67-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
jssc.start();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO67-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, <code class="literal">media_type</code> in this example
</p></td></tr></table></div><h4><a id="spark-streaming-write-meta"></a>Handling document metadata<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Elasticsearch allows each document to have its own <a class="ulink" href="http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html" target="_top">metadata</a>. As explained above, through the various <a class="link" href="configuration.html#cfg-mapping" title="Mapping (when writing to Elasticsearch)edit">mapping</a> options one can customize these parameters so that their values are extracted from their belonging document. Further more, one can even include/exclude what parts of the data are sent back to Elasticsearch. In Spark, elasticsearch-hadoop extends this functionality allowing metadata to be supplied <span class="emphasis"><em>outside</em></span> the document itself through the use of <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs" target="_top"><span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>s</a>.</p><p>This is no different in Spark Streaming. For <code class="literal">DStreams</code>s containing a key-value tuple, the metadata can be extracted from the key and the value used as the document source.</p><p>The metadata is described through the <code class="literal">Metadata</code> Java <a class="ulink" href="http://docs.oracle.com/javase/tutorial/java/javaOO/enum.html" target="_top">enum</a> within <code class="literal">org.elasticsearch.spark.rdd</code> package which identifies its type - <code class="literal">id</code>, <code class="literal">ttl</code>, <code class="literal">version</code>, etc…
Thus a <code class="literal">DStream</code>'s keys can be a <code class="literal">Map</code> containing the <code class="literal">Metadata</code> for each document and its associated values. If the <code class="literal">DStream</code> key is not of type <code class="literal">Map</code>, elasticsearch-hadoop will consider the object as representing the document id and use it accordingly.
This sounds more complicated than it is, so let us see some examples.</p><h5><a id="spark-streaming-write-meta-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Pair <code class="literal">DStreams</code>s, or simply put <code class="literal">DStreams</code>s with the signature <code class="literal">DStream[(K,V)]</code> can take advantage of the <code class="literal">saveToEsWithMeta</code> methods that are available either through the <span class="emphasis"><em>implicit</em></span> import of <code class="literal">org.elasticsearch.spark.streaming</code> package or <code class="literal">EsSparkStreaming</code> object.
To manually specify the id for each document, simply pass in the <code class="literal">Object</code> (not of type <code class="literal">Map</code>) in your <code class="literal">DStream</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// instance of SparkContext
val sc = ...
// instance of StreamingContext
val ssc = ...

val airportsRDD<a id="CO68-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span> = sc.makeRDD(Seq((1, otp), (2, muc), (3, sfo)))  <a id="CO68-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
val microbatches = mutable.Queue(airportsRDD)

ssc.queueStream<a id="CO68-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>(microbatches).saveToEsWithMeta<a id="CO68-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>("airports/2015")
ssc.start()</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO68-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">airportsRDD</code> is a <span class="emphasis"><em>key-value</em></span> pair <code class="literal">RDD</code>; it is created from a <code class="literal">Seq</code> of <code class="literal">tuple</code>s
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO68-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The key of each tuple within the <code class="literal">Seq</code> represents the <span class="emphasis"><em>id</em></span> of its associated value/document; in other words, document <code class="literal">otp</code> has id <code class="literal">1</code>, <code class="literal">muc</code> <code class="literal">2</code> and <code class="literal">sfo</code> <code class="literal">3</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO68-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
We construct a <code class="literal">DStream</code> which inherits the type signature of the <code class="literal">RDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO68-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Since the resulting <code class="literal">DStream</code> is a pair <code class="literal">DStream</code>, it has the <code class="literal">saveToEsWithMeta</code> method available. This tells elasticsearch-hadoop to pay special attention to the <code class="literal">DStream</code> keys and use them as metadata, in this case as document ids. If <code class="literal">saveToEs</code> would have been used instead, then elasticsearch-hadoop would consider the <code class="literal">DStream</code> tuple, that is both the key and the value, as part of the document.
</p></td></tr></table></div><p>When more than just the id needs to be specified, one should use a <code class="literal">scala.collection.Map</code> with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.elasticsearch.spark.rdd.Metadata._          <a id="CO69-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// metadata for each document
// note it's not required for them to have the same structure
val otpMeta = Map(ID -&gt; 1, TTL -&gt; "3h")                <a id="CO69-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
val mucMeta = Map(ID -&gt; 2, VERSION -&gt; "23")            <a id="CO69-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val sfoMeta = Map(ID -&gt; 3)                             <a id="CO69-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

// instance of SparkContext
val sc = ...
// instance of StreamingContext
val ssc = ...

val airportsRDD = sc.makeRDD<a id="CO69-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>(Seq((otpMeta, otp), (mucMeta, muc), (sfoMeta, sfo)))
val microbatches = mutable.Queue(airportsRDD)

ssc.queueStream<a id="CO69-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>(microbatches).saveToEsWithMeta<a id="CO69-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span>("airports/2015")
ssc.start()</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Import the <code class="literal">Metadata</code> enum
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">otp</code> document. In this case, <code class="literal">ID</code> with a value of 1 and <code class="literal">TTL</code> with a value of <code class="literal">3h</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">muc</code> document. In this case, <code class="literal">ID</code> with a value of 2 and <code class="literal">VERSION</code> with a value of <code class="literal">23</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">sfo</code> document. In this case, <code class="literal">ID</code> with a value of 3
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata and the documents are assembled into a <span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">DStream</code> inherits the signature from the <code class="literal">RDD</code>, becoming a pair <code class="literal">DStream</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">DStream</code> is configured to index the data accordingly using the <code class="literal">saveToEsWithMeta</code> method
</p></td></tr></table></div><h5><a id="spark-streaming-write-meta-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In a similar fashion, on the Java side, <code class="literal">JavaEsSparkStreaming</code> provides <code class="literal">saveToEsWithMeta</code> methods that are applied to <code class="literal">JavaPairDStream</code> (the equivalent in Java of <code class="literal">DStream[(K,V)]</code>).</p><p>This tends to involve a little more work due to the Java API’s limitations. For instance, you cannot create a <code class="literal">JavaPairDStream</code> directly from a queue of <code class="literal">JavaPairRDD</code>s. Instead, you must create a regular <code class="literal">JavaDStream</code> of <code class="literal">Tuple2</code> objects and convert the <code class="literal">JavaDStream</code> into a <code class="literal">JavaPairDStream</code>. This sounds complex, but it’s a simple work around for a limitation of the API.</p><p>First, we’ll create a pair function, that takes a <code class="literal">Tuple2</code> object in, and returns it right back to the framework:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">public static class ExtractTuples implements PairFunction&lt;Tuple2&lt;Object, Object&gt;, Object, Object&gt;, Serializable {
    @Override
    public Tuple2&lt;Object, Object&gt; call(Tuple2&lt;Object, Object&gt; tuple2) throws Exception {
        return tuple2;
    }
}</pre></div><p>Then we’ll apply the pair function to a <code class="literal">JavaDStream</code> of <code class="literal">Tuple2</code>s to create a <code class="literal">JavaPairDStream</code> and save it:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.streaming.api.java.JavaEsSparkStreaming;

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; jfk = ImmutableMap.of("iata", "JFK", "name", "JFK NYC");

JavaSparkContext jsc = ...
JavaStreamingContext jssc = ...

// create an RDD of between the id and the docs
JavaRDD&lt;Tuple2&lt;?, ?&gt;&gt; rdd = jsc.parallelize<a id="CO70-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(1, otp),          <a id="CO70-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(2, jfk)));        <a id="CO70-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

Queue&lt;JavaRDD&lt;Tuple2&lt;?, ?&gt;&gt;&gt; microbatches = ...
JavaDStream&lt;Tuple2&lt;?, ?&gt;&gt; dStream = jssc.queueStream(microbatches); <a id="CO70-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

JavaPairDStream&lt;?, ?&gt; pairDStream = dstream.mapToPair(new ExtractTuples()); <a id="CO70-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>

JavaEsSparkStreaming.saveToEsWithMeta(pairDStream, target);       <a id="CO70-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
jssc.start();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a regular <code class="literal">JavaRDD</code> of Scala <code class="literal">Tuple2</code>s wrapped around the document id and the document itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the first document wrapped around the id (<code class="literal">1</code>) and the doc (<code class="literal">otp</code>) itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the second document wrapped around the id (<code class="literal">2</code>) and <code class="literal">jfk</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Assemble a regular <code class="literal">JavaDStream</code> out of the tuple <code class="literal">RDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Transform the <code class="literal">JavaDStream</code> into a <code class="literal">JavaPairDStream</code> by passing our <code class="literal">Tuple2</code> identity function to the <code class="literal">mapToPair</code> method. This will allow the type to be converted to a <code class="literal">JavaPairDStream</code>. This function could be replaced by anything in your job that would extract both the id and the document to be indexed from a single entry.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">JavaPairRDD</code> is configured to index the data accordingly using the keys as a id and the values as documents
</p></td></tr></table></div><p>When more than just the id needs to be specified, one can choose to use a <code class="literal">java.util.Map</code> populated with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>. We’ll use the same typing trick to repack the <code class="literal">JavaDStream</code> as a <code class="literal">JavaPairDStream</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.streaming.api.java.JavaEsSparkStreaming;
import org.elasticsearch.spark.rdd.Metadata;          <a id="CO71-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

import static org.elasticsearch.spark.rdd.Metadata.*; <a id="CO71-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; sfo = ImmutableMap.of("iata", "SFO", "name", "San Fran");

// metadata for each document
// note it's not required for them to have the same structure
Map&lt;Metadata, Object&gt; otpMeta<a id="CO71-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt;<a id="CO71-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span> of(ID, 1, TTL, "1d");
Map&lt;Metadata, Object&gt; sfoMeta<a id="CO71-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt; of(ID, "2", VERSION, "23");

JavaSparkContext jsc = ...

// create a pair RDD between the id and the docs
JavaRDD&lt;Tuple2&lt;?, ?&gt;&gt; pairRdd = jsc.parallelize&lt;(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(otpMeta, otp),    <a id="CO71-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(sfoMeta, sfo)));  <a id="CO71-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span>

Queue&lt;JavaRDD&lt;Tuple2&lt;?, ?&gt;&gt;&gt; microbatches = ...
JavaDStream&lt;Tuple2&lt;?, ?&gt;&gt; dStream = jssc.queueStream(microbatches); <a id="CO71-8"></a><span><img src="images/icons/callouts/8.png" alt="" /></span>

JavaPairDStream&lt;?, ?&gt; pairDStream = dstream.mapToPair(new ExtractTuples()) <a id="CO71-9"></a><span><img src="images/icons/callouts/9.png" alt="" /></span>

JavaEsSparkStreaming.saveToEsWithMeta(pairDStream, target);       <a id="CO71-10"></a><span><img src="images/icons/callouts/10.png" alt="" /></span>
jssc.start();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">Metadata</code> <code class="literal">enum</code> describing the document metadata that can be declared
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
static import for the <code class="literal">enum</code> to refer to its values in short format (<code class="literal">ID</code>, <code class="literal">TTL</code>, etc…)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">otp</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Boiler-plate construct for forcing the <code class="literal">of</code> method generic signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">sfo</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple between <code class="literal">otp</code> (as the value) and its metadata (as the key)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple associating <code class="literal">sfo</code> and its metadata
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-8"><span><img src="images/icons/callouts/8.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a <code class="literal">JavaDStream</code> out of the <code class="literal">JavaRDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-9"><span><img src="images/icons/callouts/9.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Repack the <code class="literal">JavaDStream</code> into a <code class="literal">JavaPairDStream</code> by mapping the <code class="literal">Tuple2</code> identity function over it.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-10"><span><img src="images/icons/callouts/10.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">saveToEsWithMeta</code> invoked over the <code class="literal">JavaPairDStream</code> containing documents and their respective metadata
</p></td></tr></table></div><h4><a id="spark-streaming-type-conversion"></a>Spark Streaming Type Conversion<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>The elasticsearch-hadoop Spark Streaming support leverages the same type mapping as the regular Spark type mapping. The mappings are repeated here for consistency:</p><div class="table"><a id="id-1.3.16.7.71"></a><p class="title"><strong>Table 7. Scala Types Conversion Table</strong></p><div class="table-contents"><table summary="Scala Types Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Scala type </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">None</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Unit</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Nil</code></p></td><td align="center" valign="top"><p>empty <code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Some[T]</code></p></td><td align="center" valign="top"><p><code class="literal">T</code> according to the table</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Map</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Traversable</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><span class="emphasis"><em>case class</em></span></p></td><td align="center" valign="top"><p><code class="literal">object</code> (see <code class="literal">Map</code>)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Product</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr></tbody></table></div></div><br class="table-break" /><p>in addition, the following <span class="emphasis"><em>implied</em></span> conversion applies for Java types:</p><div class="table"><a id="id-1.3.16.7.73"></a><p class="title"><strong>Table 8. Java Types Conversion Table</strong></p><div class="table-contents"><table summary="Java Types Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Java type </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">null</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">String</code></p></td><td align="center" valign="top"><p><code class="literal">string</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Boolean</code></p></td><td align="center" valign="top"><p><code class="literal">boolean</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Byte</code></p></td><td align="center" valign="top"><p><code class="literal">byte</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Short</code></p></td><td align="center" valign="top"><p><code class="literal">short</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Integer</code></p></td><td align="center" valign="top"><p><code class="literal">int</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Long</code></p></td><td align="center" valign="top"><p><code class="literal">long</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Double</code></p></td><td align="center" valign="top"><p><code class="literal">double</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Float</code></p></td><td align="center" valign="top"><p><code class="literal">float</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Number</code></p></td><td align="center" valign="top"><p><code class="literal">float</code> or <code class="literal">double</code> (depending on size)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Calendar</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Date</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">java.util.Timestamp</code></p></td><td align="center" valign="top"><p><code class="literal">date</code>  (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">byte[]</code></p></td><td align="center" valign="top"><p><code class="literal">string</code> (BASE64)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Object[]</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Iterable</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">Map</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><span class="emphasis"><em>Java Bean</em></span></p></td><td align="center" valign="top"><p><code class="literal">object</code> (see <code class="literal">Map</code>)</p></td></tr></tbody></table></div></div><br class="table-break" /><p><strong>Geo types. </strong>It is worth re-mentioning that rich data types available only in Elasticsearch, such as <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-point.html" target="_top"><code class="literal">GeoPoint</code></a> or <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-shape.html" target="_top"><code class="literal">GeoShape</code></a> are supported by converting their structure into the primitives available in the table above.
For example, based on its storage a <code class="literal">geo_point</code> might be returned as a <code class="literal">String</code> or a <code class="literal">Traversable</code>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="spark-sql"></a>Spark SQL support<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 2.1. </p></div></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org/sql/" target="_top">Spark SQL</a> is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark website
</span></td></tr></table></div><p>On top of the core Spark support, elasticsearch-hadoop also provides integration with Spark SQL. In other words, Elasticsearch becomes a <span class="emphasis"><em>native</em></span> source for Spark SQL so that data can be indexed and queried from Spark SQL <span class="emphasis"><em>transparently</em></span>.</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Spark SQL works with <span class="emphasis"><em>structured</em></span> data - in other words, all entries are expected to have the <span class="emphasis"><em>same</em></span> structure (same number of fields, of the same type and name). Using unstructured data (documents with different
structures) is <span class="emphasis"><em>not</em></span> supported and will cause problems. For such cases, use <code class="literal">PairRDD</code>s.</p></div></div><h4><a id="spark-sql-versions"></a>Supported Spark SQL versions<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Spark SQL while becoming a mature component, is still going through significant changes between releases. Spark SQL became a stable component in version 1.3, however it is <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#migration-guide" target="_top"><span class="strong strong"><strong>not</strong></span> backwards compatible</a> with the previous releases. Further more Spark 2.0 introduced significant changed which broke backwards compatibility, through
the <code class="literal">Dataset</code> API.
elasticsearch-hadoop supports both version Spark SQL 1.3-1.6 and Spark SQL 2.0 through two different jars:
<code class="literal">elasticsearch-spark-1.x-&lt;version&gt;.jar</code> and <code class="literal">elasticsearch-hadoop-&lt;version&gt;.jar</code> support Spark SQL 1.3-1.6 (or higher) while <code class="literal">elasticsearch-spark-2.0-&lt;version&gt;.jar</code> supports Spark SQL 2.0.
In other words, unless you are using Spark 2.0, use <code class="literal">elasticsearch-spark-1.x-&lt;version&gt;.jar</code></p><p>Spark SQL support is available under <code class="literal">org.elasticsearch.spark.sql</code> package.</p><p><strong>API differences. </strong>From the elasticsearch-hadoop user perspectives, the differences between Spark SQL 1.3-1.6 and Spark 2.0 are fairly consolidated. This <a class="ulink" href="http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#upgrading-from-spark-sql-16-to-20" target="_top">document</a> describes at length the differences which are briefly mentioned below:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
<code class="literal">DataFrame</code> vs <code class="literal">Dataset</code>
</span></dt><dd>
The core unit of Spark SQL in 1.3+ is a <code class="literal">DataFrame</code>. This API remains in Spark 2.0 however underneath it is based on a <code class="literal">Dataset</code>
</dd><dt><span class="term">
Unified API vs dedicated Java/Scala APIs
</span></dt><dd>
In Spark SQL 2.0, the APIs are further <a class="ulink" href="http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes" target="_top">unified</a> by introducing <code class="literal">SparkSession</code> and by using the same backing code for both `Dataset`s, `DataFrame`s and `RDD`s.
</dd></dl></div><p>As conceptually, a <code class="literal">DataFrame</code> is a <code class="literal">Dataset[Row]</code>, the documentation below will focus on Spark SQL 1.3-1.6.</p><h4><a id="spark-sql-write"></a>Writing <code class="literal">DataFrame</code> (Spark SQL 1.3+) to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>With elasticsearch-hadoop, <code class="literal">DataFrame</code>s (or any <code class="literal">Dataset</code> for that matter) can be indexed to Elasticsearch.</p><h5><a id="spark-sql-write-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In Scala, simply import <code class="literal">org.elasticsearch.spark.sql</code> package which enriches the given <code class="literal">DataFrame</code> class with <code class="literal">saveToEs</code> methods; while these have the same signature as the <code class="literal">org.elasticsearch.spark</code> package, they are designed for <code class="literal">DataFrame</code> implementations:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">// reusing the example from Spark SQL documentation

import org.apache.spark.sql.SQLContext    <a id="CO72-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.sql.SQLContext._

import org.elasticsearch.spark.sql._      <a id="CO72-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

// sc = existing SparkContext
val sqlContext = new SQLContext(sc)

// case class used to define the DataFrame
case class Person(name: String, surname: String, age: Int)

//  create DataFrame
val people = sc.textFile("people.txt")    <a id="CO72-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
        .map(_.split(","))
        .map(p =&gt; Person(p(0), p(1), p(2).trim.toInt))
        .toDF()

people.saveToEs("spark/people")           <a id="CO72-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL package import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Spark package import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Read a text file as <span class="emphasis"><em>normal</em></span> <code class="literal">RDD</code> and map it to a <code class="literal">DataFrame</code> (using the <code class="literal">Person</code> case class)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Index the resulting <code class="literal">DataFrame</code> to Elasticsearch through the <code class="literal">saveToEs</code> method
</p></td></tr></table></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>By default, elasticsearch-hadoop will ignore null values in favor of not writing any field at all. Since a <code class="literal">DataFrame</code> is meant
to be treated as structured tabular data, you can enable writing nulls as null valued fields for <code class="literal">DataFrame</code> Objects
only by toggling the <code class="literal">es.spark.dataframe.write.null</code> setting to <code class="literal">true</code>.</p></div></div><h5><a id="spark-sql-write-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In a similar fashion, for Java usage the dedicated package <code class="literal">org.elasticsearch.spark.sql.api.java</code> provides similar functionality through the <code class="literal">JavaEsSpark SQL</code> :</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.sql.api.java.*;                      <a id="CO73-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.sql.api.java.JavaEsSparkSQL;  <a id="CO73-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

DataFrame people = ...
JavaEsSparkSQL.saveToEs(people, "spark/people");                     <a id="CO73-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Spark SQL Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the <code class="literal">DataFrame</code> in Elasticsearch under <code class="literal">spark/people</code>
</p></td></tr></table></div><p>Again, with Java 5 <span class="emphasis"><em>static</em></span> imports this can be further simplied to:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.sql.api.java.JavaEsSpark SQL; <a id="CO74-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
...
saveToEs("spark/people");                                          <a id="CO74-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSpark SQL</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
call <code class="literal">saveToEs</code> method without having to type <code class="literal">JavaEsSpark</code> again
</p></td></tr></table></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>For maximum control over the mapping of your <code class="literal">DataFrame</code> in Elasticsearch, it is highly recommended to create the mapping before hand. See <a class="link" href="mapping.html" title="Mapping and Types">this</a> chapter for more information.</p></div></div><h4><a id="spark-sql-json"></a>Writing existing JSON to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>When using Spark SQL, if the input data is in JSON format, simply convert it to a <code class="literal">DataFrame</code> (in Spark SQL 1.3) or a <code class="literal">Dataset</code> (for Spark SQL 2.0) (as described in Spark <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets" target="_top">documentation</a>) through <code class="literal">SQLContext</code>/<code class="literal">JavaSQLContext</code> <code class="literal">jsonFile</code> methods.</p><h4><a id="spark-sql-read-ds"></a>Using pure SQL to read from Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>The index and its mapping, have to exist prior to creating the temporary table</p></div></div><p>Spark SQL 1.2 <a class="ulink" href="http://spark.apache.org/releases/spark-release-1-2-0.html" target="_top">introduced</a> a new <a class="ulink" href="https://github.com/apache/spark/pull/2475" target="_top">API</a> for reading from external data sources, which is supported by elasticsearch-hadoop
simplifying the SQL configured needed for interacting with Elasticsearch. Further more, behind the scenes it understands the operations executed by Spark and thus can optimize the data and queries made (such as filtering or pruning),
improving performance.</p><h4><a id="spark-data-sources"></a>Data Sources in Spark SQL<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>When using Spark SQL, elasticsearch-hadoop allows access to Elasticsearch through <code class="literal">SQLContext</code> <code class="literal">load</code> method. In other words, to create a <code class="literal">DataFrame</code>/<code class="literal">Dataset</code> backed by Elasticsearch in a <span class="emphasis"><em>declarative</em></span> manner:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val sql = new SQLContext...
// Spark 1.3 style
val df = sql.load<a id="CO75-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>("spark/index"<a id="CO75-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>, "org.elasticsearch.spark.sql"<a id="CO75-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>)</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO75-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">SQLContext</code> <span class="emphasis"><em>experimental</em></span> <code class="literal">load</code> method for arbitrary data sources
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO75-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
path or resource to load - in this case the index/type in Elasticsearch
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO75-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
the data source provider - <code class="literal">org.elasticsearch.spark.sql</code>
</p></td></tr></table></div><p>In Spark 1.4, one would use the following similar API calls:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">// Spark 1.4 style
val df = sql.read<a id="CO76-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>.format("org.elasticsearch.spark.sql"<a id="CO76-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>).load("spark/index"<a id="CO76-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>)</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO76-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">SQLContext</code> <span class="emphasis"><em>experimental</em></span> <code class="literal">read</code> method for arbitrary data sources
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO76-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
the data source provider - <code class="literal">org.elasticsearch.spark.sql</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO76-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
path or resource to load - in this case the index/type in Elasticsearch
</p></td></tr></table></div><p>In Spark 1.5, this can be further simplified to:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">// Spark 1.5 style
val df = sql.read.format("es"<a id="CO77-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>).load("spark/index")</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO77-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Use <code class="literal">es</code> as an alias instead of the full package name for the <code class="literal">DataSource</code> provider
</p></td></tr></table></div><p>Whatever API is used, once created, the <code class="literal">DataFrame</code> can be accessed freely to manipulate the data.</p><p>The <span class="emphasis"><em>sources</em></span> declaration also allows specific options to be passed in, namely:</p><div class="informaltable"><table cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="center" valign="top"> Name </th><th align="center" valign="top"> Default value</th><th align="center" valign="top"> Description</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">path</code></p></td><td align="center" valign="top"><p><span class="emphasis"><em>required</em></span></p></td><td align="center" valign="top"><p>Elasticsearch index/type</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">pushdown</code></p></td><td align="center" valign="top"><p><code class="literal">true</code></p></td><td align="center" valign="top"><p>Whether to translate (<span class="emphasis"><em>push-down</em></span>) Spark SQL into Elasticsearch Query DSL</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">strict</code></p></td><td align="center" valign="top"><p><code class="literal">false</code></p></td><td align="center" valign="top"><p>Whether to use <span class="emphasis"><em>exact</em></span> (not analyzed) matching or not (analyzed)</p></td></tr><tr><td colspan="3" align="center" valign="top"><p><span class="strong strong"><strong>Usable in Spark 1.6 or higher</strong></span></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">double.filtering</code></p></td><td align="center" valign="top"><p><code class="literal">true</code></p></td><td align="center" valign="top"><p>Whether to tell Spark apply its own filtering on the filters pushed down</p></td></tr></tbody></table></div><p>Both options are explained in the next section.
To specify the options (including the generic elasticsearch-hadoop ones), one simply passes a <code class="literal">Map</code> to the aforementioned methods:</p><p>For example:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val sql = new SQLContext...
// options for Spark 1.3 need to include the target path/resource
val options13 = Map("path" -&gt; "spark/index",
                    "pushdown"<a id="CO78-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span> -&gt; "true",
                    "es.nodes"<a id="CO78-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span> -&gt; "someNode", "es.port" -&gt; "9200")

// Spark 1.3 style
val spark13DF = sql.load("org.elasticsearch.spark.sql", options13<a id="CO78-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>)

// options for Spark 1.4 - the path/resource is specified separately
val options = Map("pushdown"<a id="CO78-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span> -&gt; "true", "es.nodes"<a id="CO78-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span> -&gt; "someNode", "es.port" -&gt; "9200")

// Spark 1.4 style
val spark14DF = sql.read.format("org.elasticsearch.spark.sql")
                        .options<a id="CO78-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>(options).load("spark/index")</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO78-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> <a href="#CO78-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">pushdown</code> option - specific to Spark data sources
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO78-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> <a href="#CO78-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">es.nodes</code> configuration option
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO78-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> <a href="#CO78-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
pass the options when definition/loading the source
</p></td></tr></table></div><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">sqlContext.sql(
   "CREATE TEMPORARY TABLE myIndex    " + <a id="CO79-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
   "USING org.elasticsearch.spark.sql " + <a id="CO79-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
   "OPTIONS (<a id="CO79-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span> resource 'spark/index', nodes 'someNode')" ) "</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO79-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark’s temporary table name
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO79-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">USING</code> clause identifying the data source provider, in this case <code class="literal">org.elasticsearch.spark.sql</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO79-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop <a class="link" href="configuration.html" title="Configuration">configuration options</a>, the mandatory one being <code class="literal">resource</code>. The <code class="literal">es.</code> prefix is fixed due to the SQL parser
</p></td></tr></table></div><p>Do note that due to the SQL parser, the <code class="literal">.</code> (among other common characters used for delimiting) is not allowed; the connector tries to work around it by append the <code class="literal">es.</code> prefix automatically however this works only for specifying the configuration options with only one <code class="literal">.</code> (like <code class="literal">es.nodes</code> above). Because of this, if properties with multiple <code class="literal">.</code> are needed, one should use the <code class="literal">SQLContext.load</code> or <code class="literal">SQLContext.read</code> methods above and pass the properties as a <code class="literal">Map</code>.</p><h4><a id="spark-pushdown"></a>Push-Down operations<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>An important <span class="emphasis"><em>hidden</em></span> feature of using elasticsearch-hadoop as a Spark <code class="literal">source</code> is that the connector understand the operations performed within the <code class="literal">DataFrame</code>/SQL and, by default, will <span class="emphasis"><em>translate</em></span> them into the appropriate <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html" target="_top">QueryDSL</a>. In other words, the connector <span class="emphasis"><em>pushes</em></span> down the operations directly at the source, where the data is efficiently filtered out so that <span class="emphasis"><em>only</em></span> the required data is streamed back to Spark.
This significantly increases the queries performance and minimizes the CPU, memory and I/O on both Spark and Elasticsearch clusters as only the needed data is returned (as oppose to returning the data in bulk only to be processed and discarded by Spark).
Note the push down operations apply even when one specifies a query - the connector will <span class="emphasis"><em>enhance</em></span> it according to the specified SQL.</p><p>As a side note, elasticsearch-hadoop supports <span class="emphasis"><em>all</em></span> the `Filter`s available in Spark (1.3.0 and higher) while retaining backwards binary-compatibility with Spark 1.3.0, pushing down to full extent the SQL operations to Elasticsearch without any user interference.</p><p>Operators those have been optimized as pushdown filters:</p><div class="informaltable"><table cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="center" valign="top">SQL syntax </th><th align="center" valign="top"> ES 1.x/2.x syntax</th><th align="center" valign="top"> ES 5.x syntax</th></tr></thead><tbody><tr><td align="center" valign="top"><p>= null , is_null</p></td><td align="center" valign="top"><p>missing</p></td><td align="center" valign="top"><p>must_not.exists</p></td></tr><tr><td align="center" valign="top"><p>= (strict)</p></td><td align="center" valign="top"><p>term</p></td><td align="center" valign="top"><p>term</p></td></tr><tr><td align="center" valign="top"><p>= (not strict)</p></td><td align="center" valign="top"><p>match</p></td><td align="center" valign="top"><p>match</p></td></tr><tr><td align="center" valign="top"><p>&gt; , &lt; , &gt;= , ⇐</p></td><td align="center" valign="top"><p>range</p></td><td align="center" valign="top"><p>range</p></td></tr><tr><td align="center" valign="top"><p>is_not_null</p></td><td align="center" valign="top"><p>exists</p></td><td align="center" valign="top"><p>exists</p></td></tr><tr><td align="center" valign="top"><p>in (strict)</p></td><td align="center" valign="top"><p>terms</p></td><td align="center" valign="top"><p>terms</p></td></tr><tr><td align="center" valign="top"><p>in (not strict)</p></td><td align="center" valign="top"><p>or.filters</p></td><td align="center" valign="top"><p>bool.should</p></td></tr><tr><td align="center" valign="top"><p>and</p></td><td align="center" valign="top"><p>and.filters</p></td><td align="center" valign="top"><p>bool.filter</p></td></tr><tr><td align="center" valign="top"><p>or</p></td><td align="center" valign="top"><p>or.filters</p></td><td align="center" valign="top"><p>bool.should [bool.filter]</p></td></tr><tr><td align="center" valign="top"><p>not</p></td><td align="center" valign="top"><p>not.filter</p></td><td align="center" valign="top"><p>bool.must_not</p></td></tr><tr><td align="center" valign="top"><p>StringStartsWith</p></td><td align="center" valign="top"><p>wildcard(arg*)</p></td><td align="center" valign="top"><p>wildcard(arg*)</p></td></tr><tr><td align="center" valign="top"><p>StringEndsWith</p></td><td align="center" valign="top"><p>wildcard(*arg)</p></td><td align="center" valign="top"><p>wildcard(*arg)</p></td></tr><tr><td align="center" valign="top"><p>StringContains</p></td><td align="center" valign="top"><p>wildcard(*arg*)</p></td><td align="center" valign="top"><p>wildcard(*arg*)</p></td></tr><tr><td align="center" valign="top"><p>EqualNullSafe (strict)</p></td><td align="center" valign="top"><p>term</p></td><td align="center" valign="top"><p>term</p></td></tr><tr><td align="center" valign="top"><p>EqualNullSafe (not strict)</p></td><td align="center" valign="top"><p>match</p></td><td align="center" valign="top"><p>match</p></td></tr></tbody></table></div><p>To wit, consider the following Spark SQL:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">// as a DataFrame
val df = sqlContext.read().format("org.elasticsearch.spark.sql").load("spark/trips")

df.printSchema()
// root
//|-- departure: string (nullable = true)
//|-- arrival: string (nullable = true)
//|-- days: long (nullable = true)

val filter = df.filter(df("arrival").equalTo("OTP").and(df("days").gt(3))</pre></div><p>or in pure SQL:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-sql">CREATE TEMPORARY TABLE trips USING org.elasticsearch.spark.sql OPTIONS (path "spark/trips")
SELECT departure FROM trips WHERE arrival = "OTP" and days &gt; 3</pre></div><p>The connector translates the query into:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">{
  "query" : {
    "filtered" : {
      "query" : {
        "match_all" : {}

      },
      "filter" : {
        "and" : [{
            "query" : {
              "match" : {
                "arrival" : "OTP"
              }
            }
          }, {
            "days" : {
              "gt" : 3
            }
          }
        ]
      }
    }
  }
}</pre></div><p>Further more, the pushdown filters can work on <code class="literal">analyzed</code> terms (the default) or can be configured to be <span class="emphasis"><em>strict</em></span> and provide <code class="literal">exact</code> matches (work only on <code class="literal">not-analyzed</code> fields). Unless one manually specifies the mapping, it is highly recommended to leave the defaults as they are.  This and other topics are discussed at length in the Elasticsearch <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html" target="_top">Reference Documentation</a>.</p><p>Note that <code class="literal">double.filtering</code>, available since elasticsearch-hadoop 2.2 for Spark 1.6 or higher, allows filters that are already pushed down to Elasticsearch to be processed/evaluated by Spark as well (default) or not. Turning this feature off, especially when dealing with large data sizes speed things up. However one should pay attention to the semantics as turning this off, might return different results (depending on how the data is indexed, <code class="literal">analyzed</code> vs <code class="literal">not_analyzed</code>). In general, when turning <span class="emphasis"><em>strict</em></span> on, one can disable <code class="literal">double.filtering</code> as well.</p><h4><a id="spark-data-sources-12"></a>Data Sources as tables<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Available since Spark SQL 1.2, one can also access a data source by declaring it as a Spark temporary table (backed by elasticsearch-hadoop):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">sqlContext.sql(
   "CREATE TEMPORARY TABLE myIndex    " + <a id="CO80-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
   "USING org.elasticsearch.spark.sql " + <a id="CO80-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
   "OPTIONS (resource 'spark/index'<a id="CO80-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>, scroll_size<a id="CO80-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span> '20')" )</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO80-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark’s temporary table name
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO80-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">USING</code> clause identifying the data source provider, in this case <code class="literal">org.elasticsearch.spark.sql</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO80-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop <a class="link" href="configuration.html" title="Configuration">configuration options</a>, the mandatory one being <code class="literal">resource</code>. One can use the <code class="literal">es</code> prefix or skip it for convenience.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO80-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Since using <code class="literal">.</code> can cause syntax exceptions, one should replace it instead with <code class="literal">_</code> style. Thus, in this example <code class="literal">es.scroll.size</code> becomes <code class="literal">scroll_size</code> (as the leading <code class="literal">es</code> can be removed). Do note this only works in Spark 1.3 as the Spark 1.4 has a stricter parser. See the chapter above for more information.
</p></td></tr></table></div><p>Once defined, the schema is picked up automatically. So one can issue queries, right away:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-sql">val all = sqlContext.sql("SELECT * FROM myIndex WHERE id &lt;= 10")</pre></div><p>As elasticsearch-hadoop is aware of the queries being made, it can <span class="emphasis"><em>optimize</em></span> the requests done to Elasticsearch. For example, given the following query:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-sql">val names = sqlContext.sql("SELECT name FROM myIndex WHERE id &gt;=1 AND id &lt;= 10")</pre></div><p>it knows only the <code class="literal">name</code> and <code class="literal">id</code> fields are required (the first to be returned to the user, the second for Spark’s internal filtering) and thus will ask <span class="emphasis"><em>only</em></span> for this data, making the queries quite efficient.</p><h4><a id="spark-sql-read"></a>Reading <code class="literal">DataFrame</code>s (Spark SQL 1.3) from Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>As you might have guessed, one can define a <code class="literal">DataFrame</code> backed by Elasticsearch documents. Or even better, have them backed by a query result, effectively creating dynamic, real-time <span class="emphasis"><em>views</em></span> over your data.</p><h5><a id="spark-sql-read-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>Through the <code class="literal">org.elasticsearch.spark.sql</code> package, <code class="literal">esDF</code> methods are available on the <code class="literal">SQLContext</code> API:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.sql.SQLContext        <a id="CO81-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

import org.elasticsearch.spark.sql._          <a id="CO81-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

val sql = new SQLContext(sc)

val people = sql.esDF("spark/people")         <a id="CO81-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

// check the associated schema
println(people.schema.treeString)             <a id="CO81-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
// root
//  |-- name: string (nullable = true)
//  |-- surname: string (nullable = true)
//  |-- age: long (nullable = true)           <a id="CO81-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO81-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO81-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop SQL Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO81-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a <code class="literal">DataFrame</code> backed by the <code class="literal">spark/people</code> index in Elasticsearch
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO81-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
the <code class="literal">DataFrame</code> associated schema discovered from Elasticsearch
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO81-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
notice how the <code class="literal">age</code> field was transformed into a <code class="literal">Long</code> when using the default Elasticsearch mapping as discussed in the <a class="xref" href="mapping.html" title="Mapping and Types"><em>Mapping and Types</em></a> chapter.
</p></td></tr></table></div><p>And just as with the Spark <span class="emphasis"><em>core</em></span> support, additional parameters can be specified such as a query. This is quite a <span class="emphasis"><em>powerful</em></span> concept as one can filter the data at the source (Elasticsearch) and use Spark only on the results:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">// get only the Smiths
val smiths = sqlContext.esDF("spark/people","?q=Smith" <a id="CO82-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>)</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO82-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Elasticsearch query whose results comprise the <code class="literal">DataFrame</code>
</p></td></tr></table></div><p><strong>Controlling the <code class="literal">DataFrame</code> schema. </strong>In some cases, especially when the index in Elasticsearch contains a lot of fields, it is desireable to create a <code class="literal">DataFrame</code> that contains only a <span class="emphasis"><em>subset</em></span> of them. While one can modify the <code class="literal">DataFrame</code> (by working on its backing <code class="literal">RDD</code>) through the official Spark API or through dedicated queries, elasticsearch-hadoop allows the user to specify what fields to include and exclude from Elasticsearch when creating the <code class="literal">DataFrame</code>.</p><p>Through <code class="literal">es.read.field.include</code> and <code class="literal">es.read.field.exclude</code> properties, one can indicate what fields to include or exclude from the index mapping. The syntax is similar to that of Elasticsearch <a class="ulink" href="http://www.elastic.co/guide/en/elasticsearch/reference/5.0/search-request-source-filtering.html" target="_top">include/exclude</a>. Multiple values can be specified by using a comma. By default, no value is specified meaning all properties/fields are included and no properties/fields are excluded.</p><p>For example:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-ini"># include
es.read.field.include = *name, address.*
# exclude
es.read.field.exclude = *.created</pre></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Due to the way SparkSQL works with a <code class="literal">DataFrame</code> schema, elasticsearch-hadoop needs to be aware of what fields are returned from Elasticsearch <span class="emphasis"><em>before</em></span> executing the actual queries. While one can restrict the fields manually through the underlying Elasticsearch query, elasticsearch-hadoop is unaware of this and the results are likely to be different or worse, errors will occur. Use the properties above instead, which Elasticsearch will properly use alongside the user query.</p></div></div><h5><a id="spark-sql-read-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>For Java users, a dedicated API exists through <code class="literal">JavaEsSpark SQL</code>. It is strikingly similar to <code class="literal">EsSpark SQL</code> however it allows configuration options to be passed in through Java collections instead of Scala ones; other than that using the two is exactly the same:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.sql.api.java.JavaSQLContext;          <a id="CO83-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.sql.api.java.JavaEsSparkSQL;   <a id="CO83-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...
SQLContext sql = new SQLContext(sc);

DataFrame people = JavaEsSparkSQL.esDF(sql, "spark/people");  <a id="CO83-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO83-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO83-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO83-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a Java <code class="literal">DataFrame</code> backed by an Elasticsearch index
</p></td></tr></table></div><p>Better yet, the <code class="literal">DataFrame</code> can be backed by a query result:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">DataFrame people = JavaEsSparkSQL.esDF(sql, "spark/people", "?q=Smith"  <a id="CO84-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>);</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO84-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Elasticsearch query backing the elasticsearch-hadoop <code class="literal">DataFrame</code>
</p></td></tr></table></div><h4><a id="spark-sql-type-conversion"></a>Spark SQL Type conversion<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>When dealing with multi-value/array fields, please see <a class="link" href="mapping.html#mapping-multi-values" title="Handling array/multi-values fieldsedit">this</a> section and in particular <a class="link" href="configuration.html#cfg-field-info" title="Field information (when reading from Elasticsearch)edit">these</a> configuration options.
IMPORTANT: If automatic index creation is used, please review <a class="link" href="mapping.html#auto-mapping-type-loss">this</a> section for more information.</p></div></div><p>elasticsearch-hadoop automatically converts Spark built-in types to Elasticsearch <a class="ulink" href="http://www.elastic.co/guide/en/elasticsearch/reference/5.0/mapping-types.html" target="_top">types</a> (and back) as shown in the table below:</p><p>While Spark SQL <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types" target="_top"><code class="literal">DataType</code>s</a> have an equivalent in both Scala and Java and thus the <a class="link" href="spark.html#spark-type-conversion" title="Type conversionedit">RDD</a> conversion can apply, there are slightly different semantics - in particular with the <code class="literal">java.sql</code> types due to the way Spark SQL handles them:</p><div class="table"><a id="id-1.3.16.8.99"></a><p class="title"><strong>Table 9. Spark SQL 1.3+ Conversion Table</strong></p><div class="table-contents"><table summary="Spark SQL 1.3+ Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Spark SQL <code class="literal">DataType</code> </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">null</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ByteType</code></p></td><td align="center" valign="top"><p><code class="literal">byte</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ShortType</code></p></td><td align="center" valign="top"><p><code class="literal">short</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">IntegerType</code></p></td><td align="center" valign="top"><p><code class="literal">int</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">LongType</code></p></td><td align="center" valign="top"><p><code class="literal">long</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">FloatType</code></p></td><td align="center" valign="top"><p><code class="literal">float</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">DoubleType</code></p></td><td align="center" valign="top"><p><code class="literal">double</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">StringType</code></p></td><td align="center" valign="top"><p><code class="literal">string</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">BinaryType</code></p></td><td align="center" valign="top"><p><code class="literal">string</code> (BASE64)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">BooleanType</code></p></td><td align="center" valign="top"><p><code class="literal">boolean</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">DateType</code></p></td><td align="center" valign="top"><p><code class="literal">date</code> (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">TimestampType</code></p></td><td align="center" valign="top"><p><code class="literal">long</code> (unix time)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ArrayType</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">MapType</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">StructType</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr></tbody></table></div></div><br class="table-break" /><p><strong>Geo Types Conversion Table. </strong>In addition to the table above, for Spark SQL 1.3 or higher, elasticsearch-hadoop performs automatic schema detection for geo types, namely Elasticsearch <code class="literal">geo_point</code> and <code class="literal">geo_shape</code>.
 Since each type allows multiple formats (<code class="literal">geo_point</code> accepts latitude and longitude to be specified in 4 different ways, while <code class="literal">geo_shape</code> allows a variety of types (currently 9)) and the mapping does not provide such information, elasticsearch-hadoop will <span class="emphasis"><em>sample</em></span> the determined geo fields at startup and retrieve an arbitrary document that contains all the relevant fields; it will parse it and thus determine the necessary schema (so for example it can tell whether a <code class="literal">geo_point</code> is
 specified as a <code class="literal">StringType</code> or as an <code class="literal">ArrayType</code>).</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Since Spark SQL is strongly-typed, each geo field needs to have the same format across <span class="emphasis"><em>all</em></span> documents. Shy of that, the returned data will not fit the detected schema and thus lead to errors.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="spark-sql-streaming"></a>Spark Structured Streaming support<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 6.0. </p></div></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_top">Structured Streaming</a> provides fast,
scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark documentation
</span></td></tr></table></div><p>Released as an experimental feature in Spark 2.0, Spark Structured Streaming provides a unified streaming and batch
interface built into the Spark SQL integration. As of elasticsearch-hadoop 6.0, we provide native functionality to index streaming data
into Elasticsearch.</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Like Spark SQL, Structured Streaming works with <span class="emphasis"><em>structured</em></span> data. All entries are expected to have the
<span class="emphasis"><em>same</em></span> structure (same number of fields, of the same type and name). Using unstructured data (documents with different
structures) is <span class="emphasis"><em>not</em></span> supported and will cause problems. For such cases, use <code class="literal">DStream</code>s.</p></div></div><h4><a id="spark-sql-streaming-versions"></a>Supported Spark Structured Streaming versions<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Spark Structured Streaming is considered <span class="emphasis"><em>generally available</em></span> as of Spark v2.2.0. As such, elasticsearch-hadoop support
for Structured Streaming (available in elasticsearch-hadoop 6.0+) is only compatible with Spark versions 2.2.0 and onward. Similar
to Spark SQL before it, Structured Streaming may be subject to significant changes between releases before its
interfaces are considered <span class="emphasis"><em>stable</em></span>.</p><p>Spark Structured Streaming support is available under the <code class="literal">org.elasticsearch.spark.sql</code> and
<code class="literal">org.elasticsearch.spark.sql.streaming</code> packages. It shares a unified interface with Spark SQL in the form of the
<code class="literal">Dataset[_]</code> api. Clients can interact with streaming <code class="literal">Dataset</code>s in almost exactly the same way as regular batch
<code class="literal">Dataset</code>s with only a
<a class="ulink" href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations" target="_top">few exceptions</a>.</p><h4><a id="spark-sql-streaming-write"></a>Writing Streaming <code class="literal">Datasets</code> (Spark SQL 2.0+) to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>With elasticsearch-hadoop, Stream-backed <code class="literal">Dataset</code>s can be indexed to Elasticsearch.</p><h5><a id="spark-sql-streaming-write-scala"></a>Scala<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In Scala, to save your streaming based <code class="literal">Dataset</code>s and <code class="literal">DataFrame</code>s to Elasticsearch, simply configure the stream to
write out using the <code class="literal">"es"</code> format, like so:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">import org.apache.spark.sql.SparkSession    <a id="CO85-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

...

val spark = SparkSession.builder()
   .appName("EsStreamingExample")           <a id="CO85-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
   .getOrCreate()

// case class used to define the DataFrame
case class Person(name: String, surname: String, age: Int)

//  create DataFrame
val people = spark.readStream                   <a id="CO85-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
        .textFile("/path/to/people/files/*")    <a id="CO85-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
        .map(_.split(","))
        .map(p =&gt; Person(p(0), p(1), p(2).trim.toInt))

people.writeStream
      .option("checkpointLocation", "/save/location")      <a id="CO85-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
      .format("es")
      .start("spark/people")                               <a id="CO85-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create <code class="literal">SparkSession</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Instead of calling <code class="literal">read</code>, call <code class="literal">readStream</code> to get instance of <code class="literal">DataStreamReader</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Read a directory of text files continuously and convert them into <code class="literal">Person</code> objects
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Provide a location to save the offsets and commit logs for the streaming query
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO85-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Start the stream using the <code class="literal">"es"</code> format to index the contents of the <code class="literal">Dataset</code> continuously to Elasticsearch
</p></td></tr></table></div><div class="warning admon"><div class="icon"><img alt="Warning" src="images/icons/warning.png" /></div><div class="admon_content"><p>Spark makes no type-based differentiation between batch and streaming based <code class="literal">Dataset</code>s. While you may be
able to import the <code class="literal">org.elasticsearch.spark.sql</code> package to add <code class="literal">saveToEs</code> methods to your <code class="literal">Dataset</code> or
<code class="literal">DataFrame</code>, it will throw an illegal argument exception if those methods are called on streaming based <code class="literal">Dataset</code>s
or <code class="literal">DataFrame</code>s.</p></div></div><h5><a id="spark-sql-streaming-write-java"></a>Java<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>In a similar fashion, the <code class="literal">"es"</code> format is available for Java usage as well:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">import org.apache.spark.sql.SparkSession    <a id="CO86-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

...

SparkSession spark = SparkSession
  .builder()
  .appName("JavaStructuredNetworkWordCount")     <a id="CO86-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
  .getOrCreate();

// java bean style class
public static class PersonBean {
  private String name;
  private String surname;                        <a id="CO86-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
  private int age;

  ...
}

Dataset&lt;PersonBean&gt; people = spark.readStream()         <a id="CO86-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
        .textFile("/path/to/people/files/*")
        .map(new MapFunction&lt;String, PersonBean&gt;() {
            @Override
            public PersonBean call(String value) throws Exception {
                return someFunctionThatParsesStringToJavaBeans(value.split(","));         <a id="CO86-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
            }
        }, Encoders.&lt;PersonBean&gt;bean(PersonBean.class));

people.writeStream()
    .option("checkpointLocation", "/save/location")       <a id="CO86-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
    .format("es")
    .start("spark/people");                               <a id="CO86-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL Java imports. Can use the same session class as Scala
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create SparkSession. Can also use the legacy <code class="literal">SQLContext</code> api
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
We create a java bean class to be used as our data format
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Use the <code class="literal">readStream()</code> method to get a <code class="literal">DataStreamReader</code> to begin building our stream
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Convert our string data into our PersonBean
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Set a place to save the state of our stream
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO86-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Using the <code class="literal">"es"</code> format, we continuously index the <code class="literal">Dataset</code> in Elasticsearch under <code class="literal">spark/people</code>
</p></td></tr></table></div><h4><a id="spark-sql-streaming-json"></a>Writing existing JSON to Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>When using Spark SQL, if the input data is in JSON format, simply convert it to a <code class="literal">Dataset</code> (for Spark SQL 2.0) (as
described in Spark
<a class="ulink" href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources" target="_top">documentation</a>) through
the <code class="literal">DataStreamReader</code>'s <code class="literal">json</code> format.</p><h4><a id="spark-sql-streaming-commit-log"></a>Sink commit log in Spark Structured Streaming<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Spark Structured Streaming advertises an end-to-end fault-tolerant exactly-once processing model that is made possible
through the usage of offset checkpoints and maintaining commit logs for each streaming query. When executing a
streaming query, most sources and sinks require you to specify a "checkpointLocation" in order to persist the state of
your job. In the event of an interruption, launching a new streaming query with the same checkpoint location will
recover the state of the job and pick up where it left off. We maintain a commit log for elasticsearch-hadoop’s Elasticsearch sink
implementation in a special directory under the configured checkpoint location:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-sh">$&gt; ls /path/to/checkpoint/location
metadata  offsets/  sinks/
$&gt; ls /path/to/checkpoint/location/sinks
elasticsearch/
$&gt; ls /path/to/checkpoint/location/sinks/elasticsearch
12.compact  13  14  15 16  17  18</pre></div><p>Each file in the commit log directory corresponds to a batch id that has been committed. The log implementation
periodically compacts the logs down to avoid clutter. You can set the location for the log directory a number of ways:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Set the explicit log location with <code class="literal">es.spark.sql.streaming.sink.log.path</code> (see below).
</li><li class="listitem">
If that is not set, then the path specified by <code class="literal">checkpointLocation</code> will be used.
</li><li class="listitem">
If that is not set, then a path will be constructed by combining the value of <code class="literal">spark.sql.streaming.checkpointLocation</code>
from the SparkSession with the <code class="literal">Dataset</code>'s given query name.
</li><li class="listitem">
If no query name is present, then a random UUID will be used in the above case instead of the query name
</li><li class="listitem">
If none of the above settings are provided then the <code class="literal">start</code> call will throw an exception
</li></ol></div><p>Here is a list of configurations that affect the behavior of Elasticsearch’s commit log:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
<code class="literal">es.spark.sql.streaming.sink.log.enabled</code> (default <code class="literal">true</code>)
</span></dt><dd>
Enables or disables the commit log for a streaming job. By default, the log is enabled, and output batches with the
same batch id will be skipped to avoid double-writes. When this is set to <code class="literal">false</code>, the commit log is disabled, and
all outputs will be sent to Elasticsearch, regardless if they have been sent in a previous execution.
</dd><dt><span class="term">
<code class="literal">es.spark.sql.streaming.sink.log.path</code>
</span></dt><dd>
Sets the location to store the log data for this streaming query. If this value is not set, then the Elasticsearch sink will
store its commit logs under the path given in <code class="literal">checkpointLocation</code>. Any HDFS Client compatible URI is acceptable.
</dd><dt><span class="term">
<code class="literal">es.spark.sql.streaming.sink.log.cleanupDelay</code> (default <code class="literal">10m</code>)
</span></dt><dd>
The commit log is managed through Spark’s HDFS Client. Some HDFS compatible filesystems (like Amazon’s S3) propagate
file changes in an asynchronous manner. To get around this, after a set of log files have been compacted, the client
will wait for this amount of time before cleaning up the old files.
</dd><dt><span class="term">
<code class="literal">es.spark.sql.streaming.sink.log.deletion</code> (default <code class="literal">true</code>)
</span></dt><dd>
Determines if the log should delete old logs that are no longer needed. After every batch is committed, the client will
check to see if there are any commit logs that have been compacted and are safe to be removed. If set to <code class="literal">false</code>, the
log will skip this cleanup step, leaving behind a commit file for each batch.
</dd><dt><span class="term">
<code class="literal">es.spark.sql.streaming.sink.log.compactInterval</code> (default <code class="literal">10</code>)
</span></dt><dd>
Sets the number of batches to process before compacting the log files. By default, every 10 batches the commit log
will be compacted down into a single file that contains all previously committed batch ids.
</dd></dl></div><h4><a id="spark-sql-streaming-type-conversion"></a>Spark Structured Streaming Type conversion<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Structured Streaming uses the exact same type conversion rules as the Spark SQL integration.</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>When dealing with multi-value/array fields, please see <a class="link" href="mapping.html#mapping-multi-values" title="Handling array/multi-values fieldsedit">this</a> section and in
particular <a class="link" href="configuration.html#cfg-field-info" title="Field information (when reading from Elasticsearch)edit">these</a> configuration options.</p></div></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>If automatic index creation is used, please review <a class="link" href="mapping.html#auto-mapping-type-loss">this</a> section for more
information.</p></div></div><p>elasticsearch-hadoop automatically converts Spark built-in types to Elasticsearch <a class="ulink" href="http://www.elastic.co/guide/en/elasticsearch/reference/5.0/mapping-types.html" target="_top">types</a> as shown in the
table below:</p><p>While Spark SQL <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types" target="_top"><code class="literal">DataType</code>s</a> have an
equivalent in both Scala and Java and thus the <a class="link" href="spark.html#spark-type-conversion" title="Type conversionedit">RDD</a> conversion can apply, there are slightly
different semantics - in particular with the <code class="literal">java.sql</code> types due to the way Spark SQL handles them:</p><div class="table"><a id="id-1.3.16.9.35"></a><p class="title"><strong>Table 10. Spark SQL 1.3+ Conversion Table</strong></p><div class="table-contents"><table summary="Spark SQL 1.3+ Conversion Table" cellpadding="4px" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="center" valign="top"> Spark SQL <code class="literal">DataType</code> </th><th align="center" valign="top"> Elasticsearch type</th></tr></thead><tbody><tr><td align="center" valign="top"><p><code class="literal">null</code></p></td><td align="center" valign="top"><p><code class="literal">null</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ByteType</code></p></td><td align="center" valign="top"><p><code class="literal">byte</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ShortType</code></p></td><td align="center" valign="top"><p><code class="literal">short</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">IntegerType</code></p></td><td align="center" valign="top"><p><code class="literal">int</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">LongType</code></p></td><td align="center" valign="top"><p><code class="literal">long</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">FloatType</code></p></td><td align="center" valign="top"><p><code class="literal">float</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">DoubleType</code></p></td><td align="center" valign="top"><p><code class="literal">double</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">StringType</code></p></td><td align="center" valign="top"><p><code class="literal">string</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">BinaryType</code></p></td><td align="center" valign="top"><p><code class="literal">string</code> (BASE64)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">BooleanType</code></p></td><td align="center" valign="top"><p><code class="literal">boolean</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">DateType</code></p></td><td align="center" valign="top"><p><code class="literal">date</code> (<code class="literal">string</code> format)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">TimestampType</code></p></td><td align="center" valign="top"><p><code class="literal">long</code> (unix time)</p></td></tr><tr><td align="center" valign="top"><p><code class="literal">ArrayType</code></p></td><td align="center" valign="top"><p><code class="literal">array</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">MapType</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr><tr><td align="center" valign="top"><p><code class="literal">StructType</code></p></td><td align="center" valign="top"><p><code class="literal">object</code></p></td></tr></tbody></table></div></div><br class="table-break" /><h3><a id="spark-mr"></a>Using the Map/Reduce layer<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>Another way of using Spark with Elasticsearch is through the Map/Reduce layer, that is by leveraging the dedicated <code class="literal">Input/OuputFormat</code> in elasticsearch-hadoop. However, unless one is stuck on
elasticsearch-hadoop 2.0, we <span class="emphasis"><em>strongly</em></span> recommend using the native integration as it offers significantly better performance and flexibility.</p><h4><a id="_configuration_3"></a>Configuration<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Through elasticsearch-hadoop, Spark can integrate with Elasticsearch through its dedicated <code class="literal">InputFormat</code>, and in case of writing, through <code class="literal">OutputFormat</code>. These are described at length in the <a class="link" href="mapreduce.html" title="Map/Reduce integration">Map/Reduce</a> chapter so please refer to that for an in-depth explanation.</p><p>In short, one needs to setup a basic Hadoop <code class="literal">Configuration</code> object with the target Elasticsearch cluster and index, potentially a query, and she’s good to go.</p><p>From Spark’s perspective, the only thing required is setting up serialization - Spark relies by default on Java serialization which is convenient but fairly inefficient. This is the reason why Hadoop itself introduced its own serialization mechanism and its own types - namely <code class="literal">Writable</code>s. As such, <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code>s are required to return <code class="literal">Writables</code> which, out of the box, Spark does not understand.
The good news is, one can easily enable a different serialization (<a class="ulink" href="https://github.com/EsotericSoftware/kryo" target="_top">Kryo</a>) which handles the conversion automatically and also does this quite efficiently.</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">SparkConf sc = new SparkConf(); //.setMaster("local");
sc.set("spark.serializer", KryoSerializer.class.getName()); <a id="CO87-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

// needed only when using the Java API
JavaSparkContext jsc = new JavaSparkContext(sc);</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO87-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Enable the Kryo serialization support with Spark
</p></td></tr></table></div><p>Or if you prefer Scala</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val sc = new SparkConf(...)
sc.set("spark.serializer", classOf[KryoSerializer].getName) <a id="CO88-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO88-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Enable the Kryo serialization support with Spark
</p></td></tr></table></div><p>Note that the Kryo serialization is used as a work-around for dealing with <code class="literal">Writable</code> types; one can choose to convert the types directly (from <code class="literal">Writable</code> to <code class="literal">Serializable</code> types) - which is fine however for getting started, the one liner above seems to be the most effective.</p><h4><a id="_reading_data_from_elasticsearch_4"></a>Reading data from Elasticsearch<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>To read data, simply pass in the <code class="literal">org.elasticsearch.hadoop.mr.EsInputFormat</code> class - since it supports both the <code class="literal">old</code> and the <code class="literal">new</code> Map/Reduce APIs, you are free to use either method on <code class="literal">SparkContext</code>'s, <code class="literal">hadoopRDD</code> (which we recommend for conciseness reasons) or <code class="literal">newAPIHadoopRDD</code>. Which ever you chose, stick with it to avoid confusion and problems down the road.</p><h5><a id="_emphasis_old_emphasis_literal_org_apache_hadoop_mapred_literal_api_3"></a><span class="emphasis"><em>Old</em></span> (<code class="literal">org.apache.hadoop.mapred</code>) API<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">JobConf conf = new JobConf();                             <a id="CO89-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists");                 <a id="CO89-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*");                           <a id="CO89-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD esRDD = jsc.hadoopRDD(conf, EsInputFormat.class,
                          Text.class, MapWritable.class); <a id="CO89-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
long docCount = esRDD.count();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO89-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the old API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO89-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO89-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO89-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represents the doc id, the value the doc itself
</p></td></tr></table></div><p>The Scala version is below:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val conf = new JobConf()                                   <a id="CO90-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists")                   <a id="CO90-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*")                             <a id="CO90-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val esRDD = sc.hadoopRDD(conf,
                classOf[EsInputFormat[Text, MapWritable]], <a id="CO90-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO90-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the old API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO90-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO90-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO90-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code>
</p></td></tr></table></div><h5><a id="_emphasis_new_emphasis_literal_org_apache_hadoop_mapreduce_literal_api_3"></a><span class="emphasis"><em>New</em></span> (<code class="literal">org.apache.hadoop.mapreduce</code>) API<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>As expected, the <code class="literal">mapreduce</code> API version is strikingly similar - replace <code class="literal">hadoopRDD</code> with <code class="literal">newAPIHadoopRDD</code> and <code class="literal">JobConf</code> with <code class="literal">Configuration</code>. That’s about it.</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-java">Configuration conf = new Configuration();       <a id="CO91-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists");       <a id="CO91-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*");                 <a id="CO91-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD esRDD = jsc.newAPIHadoopRDD(conf, EsInputFormat.class,
                Text.class, MapWritable.class); <a id="CO91-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
long docCount = esRDD.count();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO91-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the new API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO91-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO91-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO91-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represent the doc id, the value the doc itself
</p></td></tr></table></div><p>The Scala version is below:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-scala">val conf = new Configuration()                             <a id="CO92-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists")                   <a id="CO92-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*")                             <a id="CO92-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val esRDD = sc.newAPIHadoopRDD(conf,
                classOf[EsInputFormat[Text, MapWritable]], <a id="CO92-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO92-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the new API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO92-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO92-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO92-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code>
</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="spark-python"></a>Using the connector from PySpark<a href="https://github.com/elastic/elasticsearch-hadoop/edit/6.4/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>Thanks to its <a class="link" href="mapreduce.html" title="Map/Reduce integration">Map/Reduce</a> layer, elasticsearch-hadoop can be used from PySpark as well to both read and write data to Elasticsearch.
To wit, below is a snippet from the <a class="ulink" href="https://spark.apache.org/docs/1.5.1/programming-guide.html#external-datasets" target="_top">Spark documentation</a> (make sure to switch to the Python snippet):</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-python">$ ./bin/pyspark --driver-class-path=/path/to/elasticsearch-hadoop.jar
&gt;&gt;&gt; conf = {"es.resource" : "index/type"}   # assume Elasticsearch is running on localhost defaults
&gt;&gt;&gt; rdd = sc.newAPIHadoopRDD("org.elasticsearch.hadoop.mr.EsInputFormat",\
    "org.apache.hadoop.io.NullWritable", "org.elasticsearch.hadoop.mr.LinkedMapWritable", conf=conf)
&gt;&gt;&gt; rdd.first()         # the result is a MapWritable that is converted to a Python dict
(u'Elasticsearch ID',
 {u'field1': True,
  u'field2': u'Some Text',
  u'field3': 12345})</pre></div><p>Also, the SQL loader can be used as well:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-python">from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.read.format("org.elasticsearch.spark.sql").load("index/type")
df.printSchema()</pre></div></div></div><div class="navfooter"><span class="prev"><a href="pig.html">
              « 
              Apache Pig support</a>
           
        </span><span class="next">
           
          <a href="storm.html">Apache Storm support
               »
            </a></span></div>
<!-- end body -->

            </div>
            <div class="col-xs-12 col-sm-4 col-md-4" id="right_col">
        
					<div id="rtpcontainer" style="display: block;">
						<div class="mktg-promo">
						<h3>Getting Started Videos</h3>
						<ul class="icons">
							<li class="icon-elasticsearch-white"><a href="https://www.elastic.co/webinars/getting-started-elasticsearch?baymax=default&elektra=docs&storm=top-video">Starting Elasticsearch</a></li>
							<li class="icon-kibana-white"><a href="https://www.elastic.co/webinars/getting-started-kibana?baymax=default&elektra=docs&storm=top-video">Introduction to Kibana</a></li>
							<li class="icon-logstash-white"><a href="https://www.elastic.co/webinars/getting-started-logstash?baymax=default&elektra=docs&storm=top-video">Logstash Starter Guide</a></li>
						</ul>
						</div>
					</div>
				</div>
			</div>
		</div>
	</section>

        </div>
      	


<div id="footer-subscribe" class="home-footer-subscribe">
  <div class="subscribe-wrapper">
    <div class="container">
      <div class="subscribe-form-container">
        <div class="subscribe-title col-md-12 col-lg-6">
            <h5>Be in the know with the latest and greatest from Elastic.</h5>
        </div>
        <div class="subscribe-form col-md-12 col-lg-6">
          <form class="mktoForm sb-form"></form>
        </div>
      </div>
      <div class="form_thanks hide">
        
          <p>Thanks for subscribing! We'll keep you updated with new releases.</p>
        
      </div>
    </div>
  </div>
</div>
<!--subscribe newsletter end-->

<!-- Footer Section -->
<footer class="footer-wrapper">
  <div class="container">
    <div class="row footer-content">
        
          <div class="col-xs-6 col-sm-3 col-md-3">
              
                <h3><a href="/products">Products ></a></h3>
              
              
              
              <ul class="">
              
                
                  
                    <li><a href="/products/elasticsearch">Elasticsearch</a></li>
                  
                
              
                
                  
                    <li><a href="/products/kibana">Kibana</a></li>
                  
                
              
                
                  
                    <li><a href="/products/beats">Beats</a></li>
                  
                
              
                
                  
                    <li><a href="/products/logstash">Logstash</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack">Stack Features (formerly X-Pack)</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/security">Security</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/alerting">Alerting</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/canvas">Canvas</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/monitoring">Monitoring</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/graph">Graph</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/reporting">Reporting</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/machine-learning">Machine Learning</a></li>
                  
                
              
                
                  
                    <li><a href="/products/stack/elasticsearch-sql">Elasticsearch SQL</a></li>
                  
                
              
                
                  
                    <li><a href="/products/hadoop">Elasticsearch-Hadoop</a></li>
                  
                
              
                
                  
                    <li><a href="/products/ece">Elastic Cloud Enterprise</a></li>
                  
                
              
              </ul>
              
                <h3><a href=""></a></h3>
              
              
              
              <ul class="">
              
              </ul>
          </div>    
      
          <div class="col-xs-6 col-sm-3 col-md-3">
              
                <h3><a href="/solutions">Solutions ></a></h3>
              
              
              
              <ul class="">
              
                
                  
                    <li><a href="/solutions/logging">Logging</a></li>
                  
                
              
                
                  
                    <li><a href="/solutions/metrics">Metrics</a></li>
                  
                
              
                
                  
                    <li><a href="/solutions/site-search">Site Search</a></li>
                  
                
              
                
                  
                    <li><a href="/solutions/security-analytics">Security Analytics</a></li>
                  
                
              
                
                  
                    <li><a href="/solutions/apm">APM</a></li>
                  
                
              
                
                  
                    <li><a href="/solutions/app-search">App Search</a></li>
                  
                
              
              </ul>
              
                <h3><a href="/cloud">Elastic Cloud ></a></h3>
              
              
              
              <ul class="">
              
                
                  
                    <li><a href="/cloud/elasticsearch-service">Elasticsearch Service</a></li>
                  
                
              
                
                  
                    <li><a href="/cloud/app-search-service">Elastic App Search Service</a></li>
                  
                
              
                
                  
                    <li><a href="/cloud/site-search-service">Elastic Site Search Service</a></li>
                  
                
              
              </ul>
          </div>    
      
          <div class="col-xs-6 col-sm-3 col-md-3">
              
                <h3>Resources</h3>
              
              
              
              <ul class="">
              
                
                  
                    <li><a href="/blog">Blog</a></li>
                  
                
              
                
                  
                    <li><a href="https://cloud-status.elastic.co">Cloud Status</a></li>
                  
                
              
                
                  
                    <li><a href="/community">Community</a></li>
                  
                
              
                
                  
                    <li><a href="/use-cases">Customers & Use Cases</a></li>
                  
                
              
                
                  
                    <li><a href="/guide">Documentation</a></li>
                  
                
              
                
                  
                    <li><a href="/elasticon">Elastic{ON} Events</a></li>
                  
                
              
                
                  
                    <li><a href="https://discuss.elastic.co/">Forums</a></li>
                  
                
              
                
                  
                    <li><a href="/community/meetups">Meetups</a></li>
                  
                
              
                
                  
                    <li><a href="/subscriptions">Subscriptions</a></li>
                  
                
              
                
                  
                    <li><a href="https://support.elastic.co">Support Portal</a></li>
                  
                
              
                
                  
                    <li><a href="/videos">Videos & Webinars</a></li>
                  
                
              
                
                  
                    <li><a href="/training">Training</a></li>
                  
                
              
              </ul>
              
                <h3></h3>
              
              
              
              <ul class="">
              
              </ul>
          </div>    
      
          <div class="col-xs-6 col-sm-3 col-md-3">
              
                <h3><a href="/about">About ></a></h3>
              
              
              
              <ul class="">
              
                
                  
                    <li><a href="/about/careers">Careers/Jobs</a></li>
                  
                
              
                
                  
                    <li><a href="/about/our-source-code">Our Source Code</a></li>
                  
                
              
                
                  
                    <li><a href="/about/teams">Teams</a></li>
                  
                
              
                
                  
                    <li><a href="/about/teams/board">Board of Directors</a></li>
                  
                
              
                
                  
                    <li><a href="/about/teams/leadership">Leadership</a></li>
                  
                
              
                
                  
                    <li><a href="/contact">Contact</a></li>
                  
                
              
                
                  
                    <li><a href="/about/history-of-elasticsearch">Our Story</a></li>
                  
                
              
                
                  
                    <li><a href="/about/why-open-source">Why Open Source</a></li>
                  
                
              
                
                  
                    <li><a href="/about/distributed">Distributed by Intention</a></li>
                  
                
              
                
                  
                    <li><a href="/about/partners">Partners</a></li>
                  
                
              
                
                  
                    <li><a href="/about/press">Media</a></li>
                  
                
              
                
                  
                    <li><a href="https://ir.elastic.co">Investor Relations</a></li>
                  
                
              
                
                  
                    <li><a href="https://www.elastic.shop">Elastic Store</a></li>
                  
                
              
              </ul>
              
                <h3><a href=""></a></h3>
              
              
              
              <ul class="">
              
              </ul>
          </div>    
       
    </div>
     <!-- Social Media Section -->
    <div class="row social-icon">
      <div class="col-xs-12 col-sm-12 col-md-6 col-md-offset-3">
	    <div class="row">
		     <div class="follow-us col-xs-12 col-sm-12 col-md-12">
			    <div class="social-label">FOLLOW US</div>
			    <ul class="social-icon-list">
          
          		 	<li class="Facebook"><a id="footer_facebook" href="https://www.facebook.com/elastic.co">Facebook</a></li>
          
          		 	<li class="Twitter"><a id="footer_twitter" href="https://www.twitter.com/elastic">Twitter</a></li>
          
          		 	<li class="LinkedIn"><a id="footer_linkedin" href="https://www.linkedin.com/company/elastic-co">LinkedIn</a></li>
          
          		 	<li class="Xing"><a id="footer_xing" href="https://www.xing.com/companies/elastic.co">Xing</a></li>
          
          		 	<li class="YouTube"><a id="footer_youtube" href="https://www.youtube.com/user/elasticsearch">YouTube</a></li>
          
		  		</ul>
		     </div>
	    </div>
      </div>
    </div>
    <!-- Social Media Section end-->
    <div class="row">
	<div class="col-xs-12 col-sm-12 col-md-11">
		<div class="copyright">
			<ul>
				<li><a href="/legal/trademarks">Trademarks</a></li>
				<li><a href="/legal/terms-of-use">Terms of Use</a></li>
				<li><a href="/legal/privacy-statement">Privacy</a></li>
				<li><a href="/brand">Brand</a></li>
			</ul>
			<div class="copyright-content">
				<div class="footer-text">
					<p>© 2018. Elasticsearch B.V. All Rights Reserved
					</p>
					<p>Elasticsearch is a trademark of Elasticsearch B.V., registered in the U.S. and in other countries.
					</p>
					<p>Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a> in the United States and/or other countries.
					</p>
				</div>
			</div>
		</div>
	</div>
	<div class="col-xs-2 col-xs-offset-5 col-sm-1 col-sm-offset-5 col-md-1 col-md-offset-0">
		<div class="footer-logo"><a href="/"><img src="/static/images/svg/logo-elastic-footer-stacked-999.svg" class="img-responsive"></a>
		</div>
	</div>
</div>
  </div>
</footer>
<!-- Footer Section end-->

<style type="text/css">
	
        
            .Facebook a {background:url("/assets/bltf546ff2e0a8f2de5/facebook.svg") no-repeat;}
            .Facebook a:before {
                content: url("/assets/bltc24cf11c714ecfc4/facebook-hover.svg");
                width:0;
                height:0;
                visibility:hidden;
            }
        
        
            .Facebook a:hover {background:url("/assets/bltc24cf11c714ecfc4/facebook-hover.svg")}		
        
	
        
            .Twitter a {background:url("/assets/bltdd9556d6b24e2b85/twitter.svg") no-repeat;}
            .Twitter a:before {
                content: url("/assets/blt90b0372cc07a54d3/twitter-hover.svg");
                width:0;
                height:0;
                visibility:hidden;
            }
        
        
            .Twitter a:hover {background:url("/assets/blt90b0372cc07a54d3/twitter-hover.svg")}		
        
	
        
            .LinkedIn a {background:url("/assets/bltc4a77e0f52ce07f1/linkedin.svg") no-repeat;}
            .LinkedIn a:before {
                content: url("/assets/blt01ce654380e4ea6c/linkedin-hover.svg");
                width:0;
                height:0;
                visibility:hidden;
            }
        
        
            .LinkedIn a:hover {background:url("/assets/blt01ce654380e4ea6c/linkedin-hover.svg")}		
        
	
        
            .Xing a {background:url("/assets/bltc6347078dcc9ec76/xing.svg") no-repeat;}
            .Xing a:before {
                content: url("/assets/bltededebe5f7521a39/xing-hover.svg");
                width:0;
                height:0;
                visibility:hidden;
            }
        
        
            .Xing a:hover {background:url("/assets/bltededebe5f7521a39/xing-hover.svg")}		
        
	
        
            .YouTube a {background:url("/assets/blt4982061ce1aebc7f/youtube.svg") no-repeat;}
            .YouTube a:before {
                content: url("/assets/bltddeb6c6404ae81a5/youtube-hover.svg");
                width:0;
                height:0;
                visibility:hidden;
            }
        
        
            .YouTube a:hover {background:url("/assets/bltddeb6c6404ae81a5/youtube-hover.svg")}		
        
	
</style>

<script src="//app-lon02.marketo.com/js/forms2/js/forms2.min.js"></script>
<script>
$(document).ready(function(){

    var ipSuccess = function(data) {
        country = data.country_code;
        if (data.in_eu){
                        
            marketo_id = "6196";
            
        }else{ 
            marketo_id = "1398";

        }

        $('.subscribe-form form.sb-form').attr('id', 'mktoForm_' + marketo_id);
        if(window.MktoForms2){
            MktoForms2.loadForm("//app-lon02.marketo.com", "813-MAM-392", marketo_id,function(form){
                form.onSuccess(function(form){
                    $('#mktoForm_'+marketo_id).css('display','none');
                    $('.subscribe-wrapper .subscribe-form-container').hide();
                    $('.subscribe-wrapper').find('.form_thanks').removeClass('hide')
                    dataLayer.push({'event': 'mktoFormSubmit'});
                    return false;
                });
            });  
        }
    }

    var marketo_id;

    $.ajax({
        url: "/gdpr-data"
    }).success(function(data) {
        ipSuccess(data);
    });

})
</script>

      </section>
    </div>

    
     <script type="text/javascript">
	var suggestionsUrl = "https://search.elastic.co/suggest";
	var localeUrl = '{"relative_url_prefix":"/","code":"en-us","display_code":"en-us","url":"/guide_template"}';
</script>
<script src="/static/js/swiftype_app_search.umd.min.js"></script>
<script src="/static/js/jquery.autocomplete.js"></script>
<script type="text/javascript" src="/static/js/slick.min.js"></script>
<script src="/static/js/nav-v5.js"></script>
<script src="/static/js/script.js"></script>
 
     
<script type="text/javascript">
$(document).ready(function(){
	if (window.location.hash.length) {
	      var target = $(window.location.hash);
	      target = target.length ? target : $(window.location.hash);
	      if (target.length) {
	        setTimeout(function(){ $(window).scrollTop(target.offset().top - 60); }, 1200);        
	      }
	}
	$('#guide').on('click','a[href*=#]:not([href=#])',function(e) {
			var flag=true;
		      if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') && location.hostname == this.hostname) {
		          var target = $(this.hash);
		          target = target.length ? target :$(this.hash);
		          if (target.length) {
		            if(flag){
		              flag=false;
		              var heightScroll=($('.tertiary-nav.navbar-fixed-top').height())?60:110;
		              $('html,body').animate({
		                  scrollTop: target.offset().top - heightScroll
		                }, 2000, function(){
		                  flag=true;
		                });
		            }                    
		          }
		      }
		    
	});

})

</script>

     <style></style>
     
  
            <script type="text/javascript" src="docs.js"></script>
<script type='text/javascript' src='https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?lang=yaml'></script>

            </body>
        
</html>
